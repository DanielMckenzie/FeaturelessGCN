{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_data import load_benchmark, load_synthetic\n",
    "from src.normalization import get_adj_feats\n",
    "from src.args import get_args\n",
    "from src.models import get_model\n",
    "from src.utils import accuracy, LDA_loss\n",
    "from src.plots import plot_feature\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading citeseer dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenye/Desktop/Dissecting_GNN_final-master/src/get_data.py:95: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish load data\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# all tensor, dense\n",
    "dataset_name = 'citeseer'\n",
    "# dataset_name = input('input dataset name: cora/citeseer/pubmed/...')\n",
    "\n",
    "adj, feats, labels, idx_train, idx_val, idx_test = load_benchmark(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get args\n",
    "# model_name = input('choose model: GCN/SGC/GFNN/GFN/AGNN/GIN/...')\n",
    "model_name = 'PreCompute_AFGNN'\n",
    "args = get_args(model_opt = model_name, dataset = dataset_name)\n",
    "norm_method = 'row'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.lr = 0.1\n",
    "# args.weight_decay = 5e-4\n",
    "# args.degree = 3\n",
    "# args.epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for SGC, return identity matrix and propagated feats\n"
     ]
    }
   ],
   "source": [
    "# get input for model\n",
    "adj, feats = get_adj_feats(adj = adj, feats = feats, model_opt = model_name, degree = args.degree, norm_method=norm_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = (torch.max(labels) + 1).numpy()\n",
    "Y_onehot =  torch.zeros(labels.shape[0], nb_class).scatter_(1, labels.unsqueeze(-1), 1)\n",
    "\n",
    "nb_each_class_train = torch.sum(Y_onehot[idx_train], dim = 0)\n",
    "nb_each_class_inv_train = torch.tensor(np.power(nb_each_class_train.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_train = torch.diag(nb_each_class_inv_train)\n",
    "\n",
    "nb_each_class_val = torch.sum(Y_onehot[idx_val], dim = 0)\n",
    "nb_each_class_inv_val = torch.tensor(np.power(nb_each_class_val.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_val = torch.diag(nb_each_class_inv_val)\n",
    "\n",
    "nb_each_class_test = torch.sum(Y_onehot[idx_test], dim = 0)\n",
    "nb_each_class_inv_test = torch.tensor(np.power(nb_each_class_test.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_test = torch.diag(nb_each_class_inv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train, test\n",
    "\n",
    "\n",
    "def train(epoch, model, optimizer, adj, feats, labels, idx_train, idx_val, \\\n",
    "          idx_test, Y_onehot, nb_each_class_inv_mat_train):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, fp1, fp2 = model(feats, adj)\n",
    "    CE_loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    if model_name == 'AGNN':\n",
    "        LDA_loss_train = LDA_loss(fp1[idx_train], Y_onehot[idx_train], nb_each_class_inv_mat_train, norm_or_not = False)\n",
    "        loss_train = CE_loss_train - LDA_loss_train\n",
    "\n",
    "    else:\n",
    "        loss_train = CE_loss_train\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output, fp1, fp2 = model(feats, adj)\n",
    "    \n",
    "    CE_loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    loss_val = CE_loss_val\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    CE_loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    loss_test = CE_loss_test\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "#           'loss_test: {:.4f}'.format(loss_test.item()),\n",
    "#           'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return epoch+1, loss_train.item(), acc_train.item(), loss_val.item(), \\\n",
    "            acc_val.item(), loss_test.item(), acc_test.item(), time.time() - t, \\\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(adj, feats, labels, idx_train, idx_val, idx_test):\n",
    "\n",
    "    # get model\n",
    "    model = get_model(model_opt = model_name, nfeat = feats.size(1), \\\n",
    "                      nclass = labels.max().item()+1, nhid = args.hidden, \\\n",
    "                      dropout = args.dropout, cuda = args.cuda, \\\n",
    "                      dataset = dataset_name, degree = args.degree)\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    if args.cuda:\n",
    "        if model_name!='AGNN' and model_name!='GIN':\n",
    "            model.cuda()\n",
    "            feats = feats.cuda()\n",
    "            adj = adj.cuda()\n",
    "            labels = labels.cuda()\n",
    "            idx_train = idx_train.cuda()\n",
    "            idx_val = idx_val.cuda()\n",
    "            idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "    # Print model's state_dict    \n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor,\"\\t\",model.state_dict()[param_tensor].size()) \n",
    "    print(\"optimizer's state_dict:\")\n",
    "\n",
    "    # Print optimizer's state_dict\n",
    "    for var_name in optimizer.state_dict():\n",
    "        print(var_name,\"\\t\",optimizer.state_dict()[var_name])\n",
    "\n",
    "    # # Print parameters\n",
    "    # for name,param in model.named_parameters():\n",
    "    #     print(name, param)\n",
    "\n",
    "\n",
    "    training_log = []\n",
    "\n",
    "    # Train model\n",
    "    t_total = time.time()\n",
    "    temp_val_loss = 999999\n",
    "    temp_test_loss = 0\n",
    "    temp_test_acc = 0\n",
    "    PATH = \"save/model_param/{}{}.pt\".format(model_name, dataset_name)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        epo, trainloss, trainacc, valloss, valacc, testloss, testacc, epotime = train(epoch, model, \\\n",
    "                                                                                      optimizer, adj, feats, \\\n",
    "                                                                                      labels, idx_train, idx_val,\\\n",
    "                                                                                      idx_test, Y_onehot, \\\n",
    "                                                                                      nb_each_class_inv_mat_train)\n",
    "        training_log.append([epo, trainloss, trainacc, valloss, valacc, testloss, testacc, epotime])\n",
    "\n",
    "        if valloss <= temp_val_loss:\n",
    "            temp_val_loss = valloss\n",
    "            temp_test_loss = testloss\n",
    "            temp_test_acc = testacc\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    print(\"Best result:\",\n",
    "              \"val_loss=\",temp_val_loss,\n",
    "                \"test_loss=\",temp_test_loss,\n",
    "                 \"test_acc=\",temp_test_acc)\n",
    "    bestmodel = torch.load(PATH)\n",
    "    if model_name == 'AGNN':\n",
    "        print(\"the weight is: \", torch.softmax(bestmodel['gc1.linear_weight'].data,dim=0))\n",
    "        \n",
    "    res_acc = temp_test_acc\n",
    "\n",
    "\n",
    "\n",
    "    # # save training log\n",
    "    # # expname = input('input experiment name: ')\n",
    "    # expname = dataset_name + '_' + model_name \n",
    "    # log_pk = open('./save/trainlog_'+expname+'.pkl','wb')\n",
    "    # pkl.dump(np.array(training_log),log_pk)\n",
    "    # log_pk.close()\n",
    "    # print(\"finish save log\")\n",
    "\n",
    "    # # store result\n",
    "\n",
    "    # X_epoch = np.array(training_log)[:,0]\n",
    "    # Y1_trainloss = np.array(training_log)[:,1]\n",
    "    # Y2_valloss = np.array(training_log)[:,3]\n",
    "    # Y3_testloss = np.array(training_log)[:,5]\n",
    "\n",
    "    # plt.plot(X_epoch, Y1_trainloss, color = 'k', label = 'train')\n",
    "    # plt.plot(X_epoch, Y2_valloss, color = 'b', label = 'val')\n",
    "    # plt.plot(X_epoch, Y3_testloss, color = 'r', linestyle = '-.', label = 'test')\n",
    "    # plt.xlabel(\"epochs\")\n",
    "    # plt.ylabel(\"loss\")\n",
    "    # plt.legend(loc = 'upper right')\n",
    "    # plt.show()\n",
    "    \n",
    "    return temp_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "W1.weight \t torch.Size([6, 3703])\n",
      "W1.bias \t torch.Size([6])\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False, 'params': [4526314984, 4526314696]}]\n",
      "Epoch: 0001 loss_train: 1.7917 acc_train: 0.1667 loss_val: 1.7870 acc_val: 0.3980 time: 0.0420s\n",
      "Epoch: 0002 loss_train: 1.7192 acc_train: 0.6583 loss_val: 1.7487 acc_val: 0.6360 time: 0.0309s\n",
      "Epoch: 0003 loss_train: 1.6657 acc_train: 0.9500 loss_val: 1.7221 acc_val: 0.5500 time: 0.0309s\n",
      "Epoch: 0004 loss_train: 1.6337 acc_train: 0.8667 loss_val: 1.7089 acc_val: 0.5500 time: 0.0436s\n",
      "Epoch: 0005 loss_train: 1.6119 acc_train: 0.8583 loss_val: 1.7035 acc_val: 0.6840 time: 0.0310s\n",
      "Epoch: 0006 loss_train: 1.5946 acc_train: 0.9083 loss_val: 1.7025 acc_val: 0.7080 time: 0.0387s\n",
      "Epoch: 0007 loss_train: 1.5819 acc_train: 0.9333 loss_val: 1.7023 acc_val: 0.6620 time: 0.0348s\n",
      "Epoch: 0008 loss_train: 1.5732 acc_train: 0.9333 loss_val: 1.6995 acc_val: 0.6500 time: 0.0318s\n",
      "Epoch: 0009 loss_train: 1.5673 acc_train: 0.9333 loss_val: 1.6937 acc_val: 0.6920 time: 0.0300s\n",
      "Epoch: 0010 loss_train: 1.5637 acc_train: 0.9167 loss_val: 1.6866 acc_val: 0.7040 time: 0.0322s\n",
      "Epoch: 0011 loss_train: 1.5625 acc_train: 0.9083 loss_val: 1.6806 acc_val: 0.7080 time: 0.0415s\n",
      "Epoch: 0012 loss_train: 1.5630 acc_train: 0.9000 loss_val: 1.6770 acc_val: 0.7180 time: 0.0319s\n",
      "Epoch: 0013 loss_train: 1.5642 acc_train: 0.8917 loss_val: 1.6761 acc_val: 0.7220 time: 0.0389s\n",
      "Epoch: 0014 loss_train: 1.5650 acc_train: 0.8917 loss_val: 1.6773 acc_val: 0.7100 time: 0.0313s\n",
      "Epoch: 0015 loss_train: 1.5649 acc_train: 0.9167 loss_val: 1.6795 acc_val: 0.7020 time: 0.0314s\n",
      "Epoch: 0016 loss_train: 1.5642 acc_train: 0.9167 loss_val: 1.6812 acc_val: 0.6960 time: 0.0338s\n",
      "Epoch: 0017 loss_train: 1.5632 acc_train: 0.9167 loss_val: 1.6812 acc_val: 0.6980 time: 0.0364s\n",
      "Epoch: 0018 loss_train: 1.5622 acc_train: 0.9167 loss_val: 1.6792 acc_val: 0.7020 time: 0.0327s\n",
      "Epoch: 0019 loss_train: 1.5615 acc_train: 0.9167 loss_val: 1.6761 acc_val: 0.7140 time: 0.0403s\n",
      "Epoch: 0020 loss_train: 1.5615 acc_train: 0.9167 loss_val: 1.6732 acc_val: 0.7160 time: 0.0338s\n",
      "Epoch: 0021 loss_train: 1.5621 acc_train: 0.9083 loss_val: 1.6714 acc_val: 0.7200 time: 0.0342s\n",
      "Epoch: 0022 loss_train: 1.5629 acc_train: 0.9000 loss_val: 1.6710 acc_val: 0.7160 time: 0.0372s\n",
      "Epoch: 0023 loss_train: 1.5634 acc_train: 0.9000 loss_val: 1.6719 acc_val: 0.7160 time: 0.0764s\n",
      "Epoch: 0024 loss_train: 1.5633 acc_train: 0.9167 loss_val: 1.6731 acc_val: 0.7160 time: 0.0617s\n",
      "Epoch: 0025 loss_train: 1.5627 acc_train: 0.9083 loss_val: 1.6737 acc_val: 0.7120 time: 0.0734s\n",
      "Epoch: 0026 loss_train: 1.5617 acc_train: 0.9083 loss_val: 1.6731 acc_val: 0.7120 time: 0.0602s\n",
      "Epoch: 0027 loss_train: 1.5605 acc_train: 0.9083 loss_val: 1.6715 acc_val: 0.7160 time: 0.0622s\n",
      "Epoch: 0028 loss_train: 1.5594 acc_train: 0.9167 loss_val: 1.6694 acc_val: 0.7140 time: 0.0601s\n",
      "Epoch: 0029 loss_train: 1.5588 acc_train: 0.9167 loss_val: 1.6679 acc_val: 0.7160 time: 0.0470s\n",
      "Epoch: 0030 loss_train: 1.5586 acc_train: 0.9167 loss_val: 1.6673 acc_val: 0.7200 time: 0.0327s\n",
      "Epoch: 0031 loss_train: 1.5588 acc_train: 0.9167 loss_val: 1.6677 acc_val: 0.7140 time: 0.0312s\n",
      "Epoch: 0032 loss_train: 1.5591 acc_train: 0.9083 loss_val: 1.6686 acc_val: 0.7140 time: 0.0319s\n",
      "Epoch: 0033 loss_train: 1.5592 acc_train: 0.9083 loss_val: 1.6694 acc_val: 0.7160 time: 0.0430s\n",
      "Epoch: 0034 loss_train: 1.5591 acc_train: 0.9083 loss_val: 1.6694 acc_val: 0.7140 time: 0.0472s\n",
      "Epoch: 0035 loss_train: 1.5587 acc_train: 0.9083 loss_val: 1.6687 acc_val: 0.7180 time: 0.0613s\n",
      "Epoch: 0036 loss_train: 1.5582 acc_train: 0.9083 loss_val: 1.6677 acc_val: 0.7220 time: 0.0510s\n",
      "Epoch: 0037 loss_train: 1.5577 acc_train: 0.9083 loss_val: 1.6669 acc_val: 0.7200 time: 0.0665s\n",
      "Epoch: 0038 loss_train: 1.5573 acc_train: 0.9000 loss_val: 1.6666 acc_val: 0.7220 time: 0.0734s\n",
      "Epoch: 0039 loss_train: 1.5570 acc_train: 0.9000 loss_val: 1.6669 acc_val: 0.7200 time: 0.0712s\n",
      "Epoch: 0040 loss_train: 1.5569 acc_train: 0.9083 loss_val: 1.6674 acc_val: 0.7240 time: 0.0811s\n",
      "Epoch: 0041 loss_train: 1.5568 acc_train: 0.9083 loss_val: 1.6677 acc_val: 0.7200 time: 0.0753s\n",
      "Epoch: 0042 loss_train: 1.5567 acc_train: 0.9083 loss_val: 1.6675 acc_val: 0.7160 time: 0.0695s\n",
      "Epoch: 0043 loss_train: 1.5565 acc_train: 0.9083 loss_val: 1.6669 acc_val: 0.7180 time: 0.0558s\n",
      "Epoch: 0044 loss_train: 1.5561 acc_train: 0.9083 loss_val: 1.6661 acc_val: 0.7160 time: 0.0625s\n",
      "Epoch: 0045 loss_train: 1.5558 acc_train: 0.9083 loss_val: 1.6656 acc_val: 0.7180 time: 0.0637s\n",
      "Epoch: 0046 loss_train: 1.5554 acc_train: 0.9083 loss_val: 1.6655 acc_val: 0.7180 time: 0.0748s\n",
      "Epoch: 0047 loss_train: 1.5552 acc_train: 0.9083 loss_val: 1.6658 acc_val: 0.7180 time: 0.0725s\n",
      "Epoch: 0048 loss_train: 1.5551 acc_train: 0.9083 loss_val: 1.6661 acc_val: 0.7180 time: 0.0578s\n",
      "Epoch: 0049 loss_train: 1.5550 acc_train: 0.9083 loss_val: 1.6661 acc_val: 0.7180 time: 0.0501s\n",
      "Epoch: 0050 loss_train: 1.5550 acc_train: 0.9083 loss_val: 1.6657 acc_val: 0.7180 time: 0.0469s\n",
      "Epoch: 0051 loss_train: 1.5550 acc_train: 0.9083 loss_val: 1.6652 acc_val: 0.7200 time: 0.0369s\n",
      "Epoch: 0052 loss_train: 1.5549 acc_train: 0.9083 loss_val: 1.6647 acc_val: 0.7260 time: 0.0318s\n",
      "Epoch: 0053 loss_train: 1.5547 acc_train: 0.9083 loss_val: 1.6644 acc_val: 0.7260 time: 0.0310s\n",
      "Epoch: 0054 loss_train: 1.5546 acc_train: 0.9083 loss_val: 1.6645 acc_val: 0.7240 time: 0.0308s\n",
      "Epoch: 0055 loss_train: 1.5544 acc_train: 0.9083 loss_val: 1.6646 acc_val: 0.7220 time: 0.0325s\n",
      "Epoch: 0056 loss_train: 1.5543 acc_train: 0.9083 loss_val: 1.6646 acc_val: 0.7200 time: 0.0319s\n",
      "Epoch: 0057 loss_train: 1.5542 acc_train: 0.9083 loss_val: 1.6643 acc_val: 0.7200 time: 0.0703s\n",
      "Epoch: 0058 loss_train: 1.5540 acc_train: 0.9167 loss_val: 1.6639 acc_val: 0.7220 time: 0.0814s\n",
      "Epoch: 0059 loss_train: 1.5539 acc_train: 0.9167 loss_val: 1.6635 acc_val: 0.7240 time: 0.0852s\n",
      "Epoch: 0060 loss_train: 1.5537 acc_train: 0.9167 loss_val: 1.6633 acc_val: 0.7260 time: 0.0705s\n",
      "Epoch: 0061 loss_train: 1.5535 acc_train: 0.9167 loss_val: 1.6633 acc_val: 0.7260 time: 0.0724s\n",
      "Epoch: 0062 loss_train: 1.5533 acc_train: 0.9083 loss_val: 1.6634 acc_val: 0.7240 time: 0.0702s\n",
      "Epoch: 0063 loss_train: 1.5531 acc_train: 0.9083 loss_val: 1.6634 acc_val: 0.7200 time: 0.0673s\n",
      "Epoch: 0064 loss_train: 1.5529 acc_train: 0.9083 loss_val: 1.6632 acc_val: 0.7200 time: 0.0787s\n",
      "Epoch: 0065 loss_train: 1.5528 acc_train: 0.9083 loss_val: 1.6630 acc_val: 0.7240 time: 0.0665s\n",
      "Epoch: 0066 loss_train: 1.5526 acc_train: 0.9083 loss_val: 1.6627 acc_val: 0.7260 time: 0.0685s\n",
      "Epoch: 0067 loss_train: 1.5525 acc_train: 0.9083 loss_val: 1.6625 acc_val: 0.7260 time: 0.0639s\n",
      "Epoch: 0068 loss_train: 1.5523 acc_train: 0.9083 loss_val: 1.6624 acc_val: 0.7220 time: 0.0417s\n",
      "Epoch: 0069 loss_train: 1.5522 acc_train: 0.9083 loss_val: 1.6625 acc_val: 0.7200 time: 0.0307s\n",
      "Epoch: 0070 loss_train: 1.5521 acc_train: 0.9083 loss_val: 1.6625 acc_val: 0.7200 time: 0.0309s\n",
      "Epoch: 0071 loss_train: 1.5520 acc_train: 0.9083 loss_val: 1.6623 acc_val: 0.7200 time: 0.0318s\n",
      "Epoch: 0072 loss_train: 1.5520 acc_train: 0.9083 loss_val: 1.6621 acc_val: 0.7260 time: 0.0402s\n",
      "Epoch: 0073 loss_train: 1.5520 acc_train: 0.9083 loss_val: 1.6619 acc_val: 0.7260 time: 0.0329s\n",
      "Epoch: 0074 loss_train: 1.5519 acc_train: 0.9167 loss_val: 1.6618 acc_val: 0.7260 time: 0.0330s\n",
      "Epoch: 0075 loss_train: 1.5519 acc_train: 0.9167 loss_val: 1.6619 acc_val: 0.7220 time: 0.0604s\n",
      "Epoch: 0076 loss_train: 1.5518 acc_train: 0.9167 loss_val: 1.6619 acc_val: 0.7200 time: 0.0726s\n",
      "Epoch: 0077 loss_train: 1.5518 acc_train: 0.9083 loss_val: 1.6619 acc_val: 0.7200 time: 0.0701s\n",
      "Epoch: 0078 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6619 acc_val: 0.7200 time: 0.0686s\n",
      "Epoch: 0079 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6617 acc_val: 0.7220 time: 0.0675s\n",
      "Epoch: 0080 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7220 time: 0.0646s\n",
      "Epoch: 0081 loss_train: 1.5516 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7220 time: 0.0694s\n",
      "Epoch: 0082 loss_train: 1.5515 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7200 time: 0.0722s\n",
      "Epoch: 0083 loss_train: 1.5515 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7200 time: 0.0640s\n",
      "Epoch: 0084 loss_train: 1.5514 acc_train: 0.9083 loss_val: 1.6615 acc_val: 0.7200 time: 0.0700s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0085 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6614 acc_val: 0.7200 time: 0.0747s\n",
      "Epoch: 0086 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6613 acc_val: 0.7200 time: 0.0621s\n",
      "Epoch: 0087 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6612 acc_val: 0.7200 time: 0.0719s\n",
      "Epoch: 0088 loss_train: 1.5512 acc_train: 0.9083 loss_val: 1.6612 acc_val: 0.7200 time: 0.0742s\n",
      "Epoch: 0089 loss_train: 1.5511 acc_train: 0.9083 loss_val: 1.6611 acc_val: 0.7200 time: 0.0723s\n",
      "Epoch: 0090 loss_train: 1.5510 acc_train: 0.9083 loss_val: 1.6611 acc_val: 0.7200 time: 0.0328s\n",
      "Epoch: 0091 loss_train: 1.5510 acc_train: 0.9083 loss_val: 1.6610 acc_val: 0.7220 time: 0.0303s\n",
      "Epoch: 0092 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6609 acc_val: 0.7220 time: 0.0310s\n",
      "Epoch: 0093 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6608 acc_val: 0.7280 time: 0.0310s\n",
      "Epoch: 0094 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6608 acc_val: 0.7280 time: 0.0402s\n",
      "Epoch: 0095 loss_train: 1.5508 acc_train: 0.9083 loss_val: 1.6608 acc_val: 0.7240 time: 0.0323s\n",
      "Epoch: 0096 loss_train: 1.5507 acc_train: 0.9083 loss_val: 1.6607 acc_val: 0.7220 time: 0.0320s\n",
      "Epoch: 0097 loss_train: 1.5507 acc_train: 0.9000 loss_val: 1.6607 acc_val: 0.7220 time: 0.0304s\n",
      "Epoch: 0098 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6606 acc_val: 0.7240 time: 0.0304s\n",
      "Epoch: 0099 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7220 time: 0.0296s\n",
      "Epoch: 0100 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7260 time: 0.0581s\n",
      "Epoch: 0101 loss_train: 1.5505 acc_train: 0.9000 loss_val: 1.6605 acc_val: 0.7260 time: 0.0371s\n",
      "Epoch: 0102 loss_train: 1.5504 acc_train: 0.9000 loss_val: 1.6604 acc_val: 0.7260 time: 0.0307s\n",
      "Epoch: 0103 loss_train: 1.5504 acc_train: 0.9000 loss_val: 1.6604 acc_val: 0.7260 time: 0.0308s\n",
      "Epoch: 0104 loss_train: 1.5504 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0295s\n",
      "Epoch: 0105 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0415s\n",
      "Epoch: 0106 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0332s\n",
      "Epoch: 0107 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7240 time: 0.0337s\n",
      "Epoch: 0108 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7240 time: 0.0577s\n",
      "Epoch: 0109 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7240 time: 0.0863s\n",
      "Epoch: 0110 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0689s\n",
      "Epoch: 0111 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0653s\n",
      "Epoch: 0112 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0659s\n",
      "Epoch: 0113 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0693s\n",
      "Epoch: 0114 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0566s\n",
      "Epoch: 0115 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0507s\n",
      "Epoch: 0116 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0567s\n",
      "Epoch: 0117 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0323s\n",
      "Epoch: 0118 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0308s\n",
      "Epoch: 0119 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0305s\n",
      "Epoch: 0120 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0305s\n",
      "Epoch: 0121 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0318s\n",
      "Epoch: 0122 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0318s\n",
      "Epoch: 0123 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0426s\n",
      "Epoch: 0124 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0320s\n",
      "Epoch: 0125 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0342s\n",
      "Epoch: 0126 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0427s\n",
      "Epoch: 0127 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0470s\n",
      "Epoch: 0128 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0504s\n",
      "Epoch: 0129 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0602s\n",
      "Epoch: 0130 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0724s\n",
      "Epoch: 0131 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0756s\n",
      "Epoch: 0132 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0940s\n",
      "Epoch: 0133 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0554s\n",
      "Epoch: 0134 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0663s\n",
      "Epoch: 0135 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0708s\n",
      "Epoch: 0136 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0449s\n",
      "Epoch: 0137 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0477s\n",
      "Epoch: 0138 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0415s\n",
      "Epoch: 0139 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0755s\n",
      "Epoch: 0140 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0634s\n",
      "Epoch: 0141 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0343s\n",
      "Epoch: 0142 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0359s\n",
      "Epoch: 0143 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0373s\n",
      "Epoch: 0144 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0333s\n",
      "Epoch: 0145 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0329s\n",
      "Epoch: 0146 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0324s\n",
      "Epoch: 0147 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0336s\n",
      "Epoch: 0148 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0357s\n",
      "Epoch: 0149 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0419s\n",
      "Epoch: 0150 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0639s\n",
      "Epoch: 0151 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0749s\n",
      "Epoch: 0152 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0803s\n",
      "Epoch: 0153 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0500s\n",
      "Epoch: 0154 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0539s\n",
      "Epoch: 0155 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0440s\n",
      "Epoch: 0156 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0465s\n",
      "Epoch: 0157 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0652s\n",
      "Epoch: 0158 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0639s\n",
      "Epoch: 0159 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0638s\n",
      "Epoch: 0160 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0485s\n",
      "Epoch: 0161 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0401s\n",
      "Epoch: 0162 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0369s\n",
      "Epoch: 0163 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0319s\n",
      "Epoch: 0164 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0322s\n",
      "Epoch: 0165 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0310s\n",
      "Epoch: 0166 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0289s\n",
      "Epoch: 0167 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0318s\n",
      "Epoch: 0168 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0436s\n",
      "Epoch: 0169 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0327s\n",
      "Epoch: 0170 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0395s\n",
      "Epoch: 0171 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0338s\n",
      "Epoch: 0172 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0475s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0780s\n",
      "Epoch: 0174 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0763s\n",
      "Epoch: 0175 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0752s\n",
      "Epoch: 0176 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0586s\n",
      "Epoch: 0177 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0632s\n",
      "Epoch: 0178 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0586s\n",
      "Epoch: 0179 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0503s\n",
      "Epoch: 0180 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0581s\n",
      "Epoch: 0181 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0584s\n",
      "Epoch: 0182 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0504s\n",
      "Epoch: 0183 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0513s\n",
      "Epoch: 0184 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0511s\n",
      "Epoch: 0185 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0495s\n",
      "Epoch: 0186 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0435s\n",
      "Epoch: 0187 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0362s\n",
      "Epoch: 0188 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0323s\n",
      "Epoch: 0189 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0424s\n",
      "Epoch: 0190 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0774s\n",
      "Epoch: 0191 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0752s\n",
      "Epoch: 0192 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0602s\n",
      "Epoch: 0193 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0702s\n",
      "Epoch: 0194 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0495s\n",
      "Epoch: 0195 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0489s\n",
      "Epoch: 0196 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0459s\n",
      "Epoch: 0197 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0670s\n",
      "Epoch: 0198 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0575s\n",
      "Epoch: 0199 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0508s\n",
      "Epoch: 0200 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0439s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 10.4341s\n",
      "Best result: val_loss= 1.659921646118164 test_loss= 1.6585285663604736 test_acc= 0.713\n",
      "Model's state_dict:\n",
      "W1.weight \t torch.Size([6, 3703])\n",
      "W1.bias \t torch.Size([6])\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False, 'params': [4526314696, 5105043064]}]\n",
      "Epoch: 0001 loss_train: 1.7919 acc_train: 0.1167 loss_val: 1.7538 acc_val: 0.4520 time: 0.0384s\n",
      "Epoch: 0002 loss_train: 1.7202 acc_train: 0.5000 loss_val: 1.7435 acc_val: 0.6900 time: 0.0325s\n",
      "Epoch: 0003 loss_train: 1.6659 acc_train: 0.9583 loss_val: 1.7411 acc_val: 0.4040 time: 0.0320s\n",
      "Epoch: 0004 loss_train: 1.6343 acc_train: 0.8667 loss_val: 1.7324 acc_val: 0.4040 time: 0.0322s\n",
      "Epoch: 0005 loss_train: 1.6127 acc_train: 0.8333 loss_val: 1.7180 acc_val: 0.4980 time: 0.0321s\n",
      "Epoch: 0006 loss_train: 1.5951 acc_train: 0.9167 loss_val: 1.7035 acc_val: 0.6960 time: 0.0393s\n",
      "Epoch: 0007 loss_train: 1.5821 acc_train: 0.9167 loss_val: 1.6925 acc_val: 0.7140 time: 0.0856s\n",
      "Epoch: 0008 loss_train: 1.5734 acc_train: 0.8917 loss_val: 1.6859 acc_val: 0.7060 time: 0.0710s\n",
      "Epoch: 0009 loss_train: 1.5676 acc_train: 0.8833 loss_val: 1.6831 acc_val: 0.7120 time: 0.0777s\n",
      "Epoch: 0010 loss_train: 1.5641 acc_train: 0.8833 loss_val: 1.6833 acc_val: 0.7120 time: 0.0742s\n",
      "Epoch: 0011 loss_train: 1.5628 acc_train: 0.9167 loss_val: 1.6850 acc_val: 0.6940 time: 0.0685s\n",
      "Epoch: 0012 loss_train: 1.5633 acc_train: 0.9167 loss_val: 1.6862 acc_val: 0.6660 time: 0.0548s\n",
      "Epoch: 0013 loss_train: 1.5645 acc_train: 0.9167 loss_val: 1.6856 acc_val: 0.6700 time: 0.0671s\n",
      "Epoch: 0014 loss_train: 1.5652 acc_train: 0.9167 loss_val: 1.6830 acc_val: 0.6860 time: 0.0550s\n",
      "Epoch: 0015 loss_train: 1.5650 acc_train: 0.9083 loss_val: 1.6794 acc_val: 0.7100 time: 0.0407s\n",
      "Epoch: 0016 loss_train: 1.5641 acc_train: 0.9083 loss_val: 1.6761 acc_val: 0.7140 time: 0.0342s\n",
      "Epoch: 0017 loss_train: 1.5630 acc_train: 0.9000 loss_val: 1.6738 acc_val: 0.7100 time: 0.0429s\n",
      "Epoch: 0018 loss_train: 1.5621 acc_train: 0.9000 loss_val: 1.6729 acc_val: 0.7040 time: 0.0331s\n",
      "Epoch: 0019 loss_train: 1.5615 acc_train: 0.9000 loss_val: 1.6733 acc_val: 0.7040 time: 0.0332s\n",
      "Epoch: 0020 loss_train: 1.5616 acc_train: 0.9167 loss_val: 1.6745 acc_val: 0.7120 time: 0.0313s\n",
      "Epoch: 0021 loss_train: 1.5622 acc_train: 0.9083 loss_val: 1.6756 acc_val: 0.6960 time: 0.0326s\n",
      "Epoch: 0022 loss_train: 1.5629 acc_train: 0.9000 loss_val: 1.6759 acc_val: 0.6920 time: 0.0315s\n",
      "Epoch: 0023 loss_train: 1.5634 acc_train: 0.9083 loss_val: 1.6751 acc_val: 0.7060 time: 0.0335s\n",
      "Epoch: 0024 loss_train: 1.5633 acc_train: 0.9167 loss_val: 1.6734 acc_val: 0.7180 time: 0.0717s\n",
      "Epoch: 0025 loss_train: 1.5626 acc_train: 0.9167 loss_val: 1.6715 acc_val: 0.7180 time: 0.0761s\n",
      "Epoch: 0026 loss_train: 1.5615 acc_train: 0.9083 loss_val: 1.6699 acc_val: 0.7200 time: 0.0387s\n",
      "Epoch: 0027 loss_train: 1.5603 acc_train: 0.9083 loss_val: 1.6690 acc_val: 0.7160 time: 0.0427s\n",
      "Epoch: 0028 loss_train: 1.5594 acc_train: 0.9083 loss_val: 1.6688 acc_val: 0.7080 time: 0.0319s\n",
      "Epoch: 0029 loss_train: 1.5588 acc_train: 0.9083 loss_val: 1.6691 acc_val: 0.7140 time: 0.0318s\n",
      "Epoch: 0030 loss_train: 1.5587 acc_train: 0.9083 loss_val: 1.6695 acc_val: 0.7220 time: 0.0304s\n",
      "Epoch: 0031 loss_train: 1.5589 acc_train: 0.9167 loss_val: 1.6697 acc_val: 0.7180 time: 0.0313s\n",
      "Epoch: 0032 loss_train: 1.5591 acc_train: 0.9167 loss_val: 1.6694 acc_val: 0.7160 time: 0.0336s\n",
      "Epoch: 0033 loss_train: 1.5592 acc_train: 0.9083 loss_val: 1.6688 acc_val: 0.7180 time: 0.0382s\n",
      "Epoch: 0034 loss_train: 1.5591 acc_train: 0.9083 loss_val: 1.6681 acc_val: 0.7180 time: 0.0486s\n",
      "Epoch: 0035 loss_train: 1.5587 acc_train: 0.9083 loss_val: 1.6676 acc_val: 0.7220 time: 0.0385s\n",
      "Epoch: 0036 loss_train: 1.5582 acc_train: 0.9083 loss_val: 1.6674 acc_val: 0.7160 time: 0.0332s\n",
      "Epoch: 0037 loss_train: 1.5577 acc_train: 0.9083 loss_val: 1.6673 acc_val: 0.7160 time: 0.0316s\n",
      "Epoch: 0038 loss_train: 1.5573 acc_train: 0.9000 loss_val: 1.6673 acc_val: 0.7180 time: 0.0294s\n",
      "Epoch: 0039 loss_train: 1.5571 acc_train: 0.9000 loss_val: 1.6673 acc_val: 0.7180 time: 0.0366s\n",
      "Epoch: 0040 loss_train: 1.5570 acc_train: 0.9083 loss_val: 1.6671 acc_val: 0.7240 time: 0.0520s\n",
      "Epoch: 0041 loss_train: 1.5569 acc_train: 0.9083 loss_val: 1.6669 acc_val: 0.7160 time: 0.0771s\n",
      "Epoch: 0042 loss_train: 1.5567 acc_train: 0.9083 loss_val: 1.6667 acc_val: 0.7160 time: 0.0741s\n",
      "Epoch: 0043 loss_train: 1.5564 acc_train: 0.9083 loss_val: 1.6666 acc_val: 0.7200 time: 0.0626s\n",
      "Epoch: 0044 loss_train: 1.5561 acc_train: 0.9083 loss_val: 1.6665 acc_val: 0.7160 time: 0.0535s\n",
      "Epoch: 0045 loss_train: 1.5557 acc_train: 0.9083 loss_val: 1.6664 acc_val: 0.7180 time: 0.0576s\n",
      "Epoch: 0046 loss_train: 1.5554 acc_train: 0.9083 loss_val: 1.6663 acc_val: 0.7160 time: 0.0539s\n",
      "Epoch: 0047 loss_train: 1.5552 acc_train: 0.9083 loss_val: 1.6660 acc_val: 0.7180 time: 0.0549s\n",
      "Epoch: 0048 loss_train: 1.5551 acc_train: 0.9083 loss_val: 1.6657 acc_val: 0.7200 time: 0.0618s\n",
      "Epoch: 0049 loss_train: 1.5550 acc_train: 0.9083 loss_val: 1.6654 acc_val: 0.7200 time: 0.0535s\n",
      "Epoch: 0050 loss_train: 1.5550 acc_train: 0.9083 loss_val: 1.6652 acc_val: 0.7200 time: 0.0634s\n",
      "Epoch: 0051 loss_train: 1.5549 acc_train: 0.9083 loss_val: 1.6652 acc_val: 0.7220 time: 0.0588s\n",
      "Epoch: 0052 loss_train: 1.5548 acc_train: 0.9167 loss_val: 1.6651 acc_val: 0.7280 time: 0.0631s\n",
      "Epoch: 0053 loss_train: 1.5547 acc_train: 0.9167 loss_val: 1.6650 acc_val: 0.7280 time: 0.0639s\n",
      "Epoch: 0054 loss_train: 1.5546 acc_train: 0.9167 loss_val: 1.6647 acc_val: 0.7220 time: 0.0562s\n",
      "Epoch: 0055 loss_train: 1.5544 acc_train: 0.9167 loss_val: 1.6644 acc_val: 0.7220 time: 0.0609s\n",
      "Epoch: 0056 loss_train: 1.5543 acc_train: 0.9083 loss_val: 1.6641 acc_val: 0.7220 time: 0.0612s\n",
      "Epoch: 0057 loss_train: 1.5542 acc_train: 0.9083 loss_val: 1.6638 acc_val: 0.7240 time: 0.0549s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0058 loss_train: 1.5540 acc_train: 0.9083 loss_val: 1.6637 acc_val: 0.7240 time: 0.0394s\n",
      "Epoch: 0059 loss_train: 1.5539 acc_train: 0.9083 loss_val: 1.6637 acc_val: 0.7240 time: 0.0371s\n",
      "Epoch: 0060 loss_train: 1.5537 acc_train: 0.9167 loss_val: 1.6637 acc_val: 0.7240 time: 0.0328s\n",
      "Epoch: 0061 loss_train: 1.5535 acc_train: 0.9167 loss_val: 1.6636 acc_val: 0.7220 time: 0.0434s\n",
      "Epoch: 0062 loss_train: 1.5533 acc_train: 0.9167 loss_val: 1.6634 acc_val: 0.7200 time: 0.0323s\n",
      "Epoch: 0063 loss_train: 1.5531 acc_train: 0.9083 loss_val: 1.6632 acc_val: 0.7220 time: 0.0625s\n",
      "Epoch: 0064 loss_train: 1.5529 acc_train: 0.9083 loss_val: 1.6629 acc_val: 0.7240 time: 0.0663s\n",
      "Epoch: 0065 loss_train: 1.5528 acc_train: 0.9083 loss_val: 1.6628 acc_val: 0.7260 time: 0.0648s\n",
      "Epoch: 0066 loss_train: 1.5527 acc_train: 0.9083 loss_val: 1.6627 acc_val: 0.7260 time: 0.0574s\n",
      "Epoch: 0067 loss_train: 1.5525 acc_train: 0.9083 loss_val: 1.6627 acc_val: 0.7260 time: 0.0677s\n",
      "Epoch: 0068 loss_train: 1.5524 acc_train: 0.9083 loss_val: 1.6626 acc_val: 0.7260 time: 0.0546s\n",
      "Epoch: 0069 loss_train: 1.5523 acc_train: 0.9083 loss_val: 1.6625 acc_val: 0.7200 time: 0.0626s\n",
      "Epoch: 0070 loss_train: 1.5522 acc_train: 0.9083 loss_val: 1.6623 acc_val: 0.7200 time: 0.0520s\n",
      "Epoch: 0071 loss_train: 1.5521 acc_train: 0.9083 loss_val: 1.6622 acc_val: 0.7220 time: 0.0621s\n",
      "Epoch: 0072 loss_train: 1.5520 acc_train: 0.9083 loss_val: 1.6621 acc_val: 0.7220 time: 0.0567s\n",
      "Epoch: 0073 loss_train: 1.5520 acc_train: 0.9083 loss_val: 1.6620 acc_val: 0.7220 time: 0.0446s\n",
      "Epoch: 0074 loss_train: 1.5519 acc_train: 0.9167 loss_val: 1.6620 acc_val: 0.7260 time: 0.0505s\n",
      "Epoch: 0075 loss_train: 1.5518 acc_train: 0.9167 loss_val: 1.6620 acc_val: 0.7200 time: 0.0378s\n",
      "Epoch: 0076 loss_train: 1.5518 acc_train: 0.9167 loss_val: 1.6619 acc_val: 0.7200 time: 0.0487s\n",
      "Epoch: 0077 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6618 acc_val: 0.7220 time: 0.0333s\n",
      "Epoch: 0078 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6617 acc_val: 0.7220 time: 0.0352s\n",
      "Epoch: 0079 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6617 acc_val: 0.7220 time: 0.0554s\n",
      "Epoch: 0080 loss_train: 1.5516 acc_train: 0.9083 loss_val: 1.6617 acc_val: 0.7200 time: 0.0616s\n",
      "Epoch: 0081 loss_train: 1.5516 acc_train: 0.9167 loss_val: 1.6616 acc_val: 0.7200 time: 0.0651s\n",
      "Epoch: 0082 loss_train: 1.5515 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7200 time: 0.0621s\n",
      "Epoch: 0083 loss_train: 1.5515 acc_train: 0.9083 loss_val: 1.6615 acc_val: 0.7200 time: 0.0583s\n",
      "Epoch: 0084 loss_train: 1.5514 acc_train: 0.9083 loss_val: 1.6614 acc_val: 0.7200 time: 0.0350s\n",
      "Epoch: 0085 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6614 acc_val: 0.7200 time: 0.0344s\n",
      "Epoch: 0086 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6613 acc_val: 0.7200 time: 0.0675s\n",
      "Epoch: 0087 loss_train: 1.5512 acc_train: 0.9083 loss_val: 1.6613 acc_val: 0.7200 time: 0.0722s\n",
      "Epoch: 0088 loss_train: 1.5512 acc_train: 0.9083 loss_val: 1.6612 acc_val: 0.7200 time: 0.0682s\n",
      "Epoch: 0089 loss_train: 1.5511 acc_train: 0.9083 loss_val: 1.6611 acc_val: 0.7220 time: 0.0642s\n",
      "Epoch: 0090 loss_train: 1.5510 acc_train: 0.9083 loss_val: 1.6610 acc_val: 0.7220 time: 0.0552s\n",
      "Epoch: 0091 loss_train: 1.5510 acc_train: 0.9083 loss_val: 1.6610 acc_val: 0.7220 time: 0.0578s\n",
      "Epoch: 0092 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6609 acc_val: 0.7220 time: 0.0532s\n",
      "Epoch: 0093 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6609 acc_val: 0.7240 time: 0.0627s\n",
      "Epoch: 0094 loss_train: 1.5508 acc_train: 0.9083 loss_val: 1.6608 acc_val: 0.7240 time: 0.0384s\n",
      "Epoch: 0095 loss_train: 1.5508 acc_train: 0.9083 loss_val: 1.6608 acc_val: 0.7240 time: 0.0389s\n",
      "Epoch: 0096 loss_train: 1.5507 acc_train: 0.9083 loss_val: 1.6607 acc_val: 0.7240 time: 0.0410s\n",
      "Epoch: 0097 loss_train: 1.5507 acc_train: 0.9167 loss_val: 1.6607 acc_val: 0.7240 time: 0.0449s\n",
      "Epoch: 0098 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6606 acc_val: 0.7220 time: 0.0388s\n",
      "Epoch: 0099 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7220 time: 0.0387s\n",
      "Epoch: 0100 loss_train: 1.5505 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7220 time: 0.0383s\n",
      "Epoch: 0101 loss_train: 1.5505 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7260 time: 0.0396s\n",
      "Epoch: 0102 loss_train: 1.5505 acc_train: 0.9083 loss_val: 1.6604 acc_val: 0.7260 time: 0.0387s\n",
      "Epoch: 0103 loss_train: 1.5504 acc_train: 0.9083 loss_val: 1.6604 acc_val: 0.7260 time: 0.0463s\n",
      "Epoch: 0104 loss_train: 1.5504 acc_train: 0.9083 loss_val: 1.6603 acc_val: 0.7260 time: 0.0378s\n",
      "Epoch: 0105 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0431s\n",
      "Epoch: 0106 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0432s\n",
      "Epoch: 0107 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0504s\n",
      "Epoch: 0108 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0428s\n",
      "Epoch: 0109 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0459s\n",
      "Epoch: 0110 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0373s\n",
      "Epoch: 0111 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0323s\n",
      "Epoch: 0112 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0308s\n",
      "Epoch: 0113 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0424s\n",
      "Epoch: 0114 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0469s\n",
      "Epoch: 0115 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0398s\n",
      "Epoch: 0116 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0407s\n",
      "Epoch: 0117 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0395s\n",
      "Epoch: 0118 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0488s\n",
      "Epoch: 0119 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0531s\n",
      "Epoch: 0120 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0431s\n",
      "Epoch: 0121 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0328s\n",
      "Epoch: 0122 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0322s\n",
      "Epoch: 0123 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0319s\n",
      "Epoch: 0124 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0385s\n",
      "Epoch: 0125 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0312s\n",
      "Epoch: 0126 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0313s\n",
      "Epoch: 0127 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0296s\n",
      "Epoch: 0128 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0291s\n",
      "Epoch: 0129 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7200 time: 0.0297s\n",
      "Epoch: 0130 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7200 time: 0.0291s\n",
      "Epoch: 0131 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7200 time: 0.0370s\n",
      "Epoch: 0132 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7200 time: 0.0318s\n",
      "Epoch: 0133 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0328s\n",
      "Epoch: 0134 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0371s\n",
      "Epoch: 0135 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0310s\n",
      "Epoch: 0136 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0297s\n",
      "Epoch: 0137 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0297s\n",
      "Epoch: 0138 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0373s\n",
      "Epoch: 0139 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0315s\n",
      "Epoch: 0140 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0312s\n",
      "Epoch: 0141 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0306s\n",
      "Epoch: 0142 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0302s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0143 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0290s\n",
      "Epoch: 0144 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0375s\n",
      "Epoch: 0145 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0305s\n",
      "Epoch: 0146 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0317s\n",
      "Epoch: 0147 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0300s\n",
      "Epoch: 0148 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0300s\n",
      "Epoch: 0149 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0317s\n",
      "Epoch: 0150 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0434s\n",
      "Epoch: 0151 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0310s\n",
      "Epoch: 0152 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0313s\n",
      "Epoch: 0153 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0305s\n",
      "Epoch: 0154 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0295s\n",
      "Epoch: 0155 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0295s\n",
      "Epoch: 0156 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0289s\n",
      "Epoch: 0157 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0327s\n",
      "Epoch: 0158 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0371s\n",
      "Epoch: 0159 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0358s\n",
      "Epoch: 0160 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0381s\n",
      "Epoch: 0161 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0398s\n",
      "Epoch: 0162 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0384s\n",
      "Epoch: 0163 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0463s\n",
      "Epoch: 0164 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0428s\n",
      "Epoch: 0165 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0387s\n",
      "Epoch: 0166 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0384s\n",
      "Epoch: 0167 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0380s\n",
      "Epoch: 0168 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0379s\n",
      "Epoch: 0169 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0464s\n",
      "Epoch: 0170 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0387s\n",
      "Epoch: 0171 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0326s\n",
      "Epoch: 0172 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0328s\n",
      "Epoch: 0173 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0349s\n",
      "Epoch: 0174 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0351s\n",
      "Epoch: 0175 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0397s\n",
      "Epoch: 0176 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0348s\n",
      "Epoch: 0177 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0347s\n",
      "Epoch: 0178 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0402s\n",
      "Epoch: 0179 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0339s\n",
      "Epoch: 0180 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0465s\n",
      "Epoch: 0181 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0535s\n",
      "Epoch: 0182 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0349s\n",
      "Epoch: 0183 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0361s\n",
      "Epoch: 0184 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0344s\n",
      "Epoch: 0185 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0326s\n",
      "Epoch: 0186 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0418s\n",
      "Epoch: 0187 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0402s\n",
      "Epoch: 0188 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0469s\n",
      "Epoch: 0189 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0545s\n",
      "Epoch: 0190 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0515s\n",
      "Epoch: 0191 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0487s\n",
      "Epoch: 0192 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0475s\n",
      "Epoch: 0193 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0391s\n",
      "Epoch: 0194 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0444s\n",
      "Epoch: 0195 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0455s\n",
      "Epoch: 0196 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0428s\n",
      "Epoch: 0197 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0399s\n",
      "Epoch: 0198 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0431s\n",
      "Epoch: 0199 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0348s\n",
      "Epoch: 0200 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0382s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 9.1395s\n",
      "Best result: val_loss= 1.6599222421646118 test_loss= 1.6585285663604736 test_acc= 0.713\n",
      "Model's state_dict:\n",
      "W1.weight \t torch.Size([6, 3703])\n",
      "W1.bias \t torch.Size([6])\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False, 'params': [5105043064, 5105001528]}]\n",
      "Epoch: 0001 loss_train: 1.7918 acc_train: 0.1833 loss_val: 1.7714 acc_val: 0.4620 time: 0.0414s\n",
      "Epoch: 0002 loss_train: 1.7194 acc_train: 0.6583 loss_val: 1.7465 acc_val: 0.6160 time: 0.0492s\n",
      "Epoch: 0003 loss_train: 1.6657 acc_train: 0.9583 loss_val: 1.7307 acc_val: 0.4740 time: 0.0461s\n",
      "Epoch: 0004 loss_train: 1.6337 acc_train: 0.8667 loss_val: 1.7188 acc_val: 0.4700 time: 0.0487s\n",
      "Epoch: 0005 loss_train: 1.6119 acc_train: 0.8167 loss_val: 1.7092 acc_val: 0.6360 time: 0.0443s\n",
      "Epoch: 0006 loss_train: 1.5945 acc_train: 0.9083 loss_val: 1.7025 acc_val: 0.6940 time: 0.0503s\n",
      "Epoch: 0007 loss_train: 1.5818 acc_train: 0.9250 loss_val: 1.6981 acc_val: 0.6500 time: 0.0400s\n",
      "Epoch: 0008 loss_train: 1.5731 acc_train: 0.9250 loss_val: 1.6940 acc_val: 0.6460 time: 0.0444s\n",
      "Epoch: 0009 loss_train: 1.5673 acc_train: 0.9333 loss_val: 1.6896 acc_val: 0.6740 time: 0.0378s\n",
      "Epoch: 0010 loss_train: 1.5638 acc_train: 0.9417 loss_val: 1.6857 acc_val: 0.7080 time: 0.0368s\n",
      "Epoch: 0011 loss_train: 1.5626 acc_train: 0.9167 loss_val: 1.6829 acc_val: 0.7240 time: 0.0340s\n",
      "Epoch: 0012 loss_train: 1.5632 acc_train: 0.9167 loss_val: 1.6811 acc_val: 0.7100 time: 0.0420s\n",
      "Epoch: 0013 loss_train: 1.5644 acc_train: 0.8917 loss_val: 1.6800 acc_val: 0.7160 time: 0.0349s\n",
      "Epoch: 0014 loss_train: 1.5652 acc_train: 0.8833 loss_val: 1.6794 acc_val: 0.7160 time: 0.0364s\n",
      "Epoch: 0015 loss_train: 1.5650 acc_train: 0.9083 loss_val: 1.6790 acc_val: 0.6980 time: 0.0360s\n",
      "Epoch: 0016 loss_train: 1.5642 acc_train: 0.9083 loss_val: 1.6786 acc_val: 0.6980 time: 0.0434s\n",
      "Epoch: 0017 loss_train: 1.5631 acc_train: 0.9083 loss_val: 1.6777 acc_val: 0.6980 time: 0.0428s\n",
      "Epoch: 0018 loss_train: 1.5621 acc_train: 0.9167 loss_val: 1.6764 acc_val: 0.7020 time: 0.0355s\n",
      "Epoch: 0019 loss_train: 1.5614 acc_train: 0.9167 loss_val: 1.6750 acc_val: 0.7040 time: 0.0330s\n",
      "Epoch: 0020 loss_train: 1.5614 acc_train: 0.9250 loss_val: 1.6740 acc_val: 0.7240 time: 0.0331s\n",
      "Epoch: 0021 loss_train: 1.5620 acc_train: 0.9167 loss_val: 1.6734 acc_val: 0.7280 time: 0.0324s\n",
      "Epoch: 0022 loss_train: 1.5628 acc_train: 0.9167 loss_val: 1.6732 acc_val: 0.7260 time: 0.0359s\n",
      "Epoch: 0023 loss_train: 1.5634 acc_train: 0.9167 loss_val: 1.6731 acc_val: 0.7260 time: 0.0395s\n",
      "Epoch: 0024 loss_train: 1.5633 acc_train: 0.9167 loss_val: 1.6729 acc_val: 0.7140 time: 0.0390s\n",
      "Epoch: 0025 loss_train: 1.5628 acc_train: 0.9167 loss_val: 1.6724 acc_val: 0.7060 time: 0.0364s\n",
      "Epoch: 0026 loss_train: 1.5618 acc_train: 0.9000 loss_val: 1.6716 acc_val: 0.7120 time: 0.0313s\n",
      "Epoch: 0027 loss_train: 1.5607 acc_train: 0.9000 loss_val: 1.6705 acc_val: 0.7120 time: 0.0317s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0028 loss_train: 1.5597 acc_train: 0.9083 loss_val: 1.6696 acc_val: 0.7120 time: 0.0347s\n",
      "Epoch: 0029 loss_train: 1.5591 acc_train: 0.9083 loss_val: 1.6690 acc_val: 0.7220 time: 0.0357s\n",
      "Epoch: 0030 loss_train: 1.5589 acc_train: 0.9167 loss_val: 1.6688 acc_val: 0.7300 time: 0.0309s\n",
      "Epoch: 0031 loss_train: 1.5591 acc_train: 0.9167 loss_val: 1.6689 acc_val: 0.7200 time: 0.0303s\n",
      "Epoch: 0032 loss_train: 1.5593 acc_train: 0.9083 loss_val: 1.6689 acc_val: 0.7160 time: 0.0307s\n",
      "Epoch: 0033 loss_train: 1.5593 acc_train: 0.9083 loss_val: 1.6688 acc_val: 0.7100 time: 0.0318s\n",
      "Epoch: 0034 loss_train: 1.5592 acc_train: 0.9083 loss_val: 1.6685 acc_val: 0.7080 time: 0.0298s\n",
      "Epoch: 0035 loss_train: 1.5588 acc_train: 0.9083 loss_val: 1.6681 acc_val: 0.7060 time: 0.0372s\n",
      "Epoch: 0036 loss_train: 1.5583 acc_train: 0.9083 loss_val: 1.6677 acc_val: 0.7160 time: 0.0306s\n",
      "Epoch: 0037 loss_train: 1.5577 acc_train: 0.9083 loss_val: 1.6675 acc_val: 0.7260 time: 0.0318s\n",
      "Epoch: 0038 loss_train: 1.5573 acc_train: 0.9083 loss_val: 1.6673 acc_val: 0.7220 time: 0.0299s\n",
      "Epoch: 0039 loss_train: 1.5570 acc_train: 0.9083 loss_val: 1.6672 acc_val: 0.7220 time: 0.0288s\n",
      "Epoch: 0040 loss_train: 1.5569 acc_train: 0.9083 loss_val: 1.6672 acc_val: 0.7200 time: 0.0300s\n",
      "Epoch: 0041 loss_train: 1.5568 acc_train: 0.9083 loss_val: 1.6670 acc_val: 0.7140 time: 0.0303s\n",
      "Epoch: 0042 loss_train: 1.5566 acc_train: 0.9083 loss_val: 1.6669 acc_val: 0.7120 time: 0.0367s\n",
      "Epoch: 0043 loss_train: 1.5564 acc_train: 0.9083 loss_val: 1.6667 acc_val: 0.7140 time: 0.0310s\n",
      "Epoch: 0044 loss_train: 1.5561 acc_train: 0.9083 loss_val: 1.6665 acc_val: 0.7200 time: 0.0304s\n",
      "Epoch: 0045 loss_train: 1.5558 acc_train: 0.9083 loss_val: 1.6663 acc_val: 0.7180 time: 0.0300s\n",
      "Epoch: 0046 loss_train: 1.5555 acc_train: 0.9083 loss_val: 1.6661 acc_val: 0.7200 time: 0.0302s\n",
      "Epoch: 0047 loss_train: 1.5553 acc_train: 0.9083 loss_val: 1.6659 acc_val: 0.7220 time: 0.0298s\n",
      "Epoch: 0048 loss_train: 1.5552 acc_train: 0.9083 loss_val: 1.6658 acc_val: 0.7180 time: 0.0300s\n",
      "Epoch: 0049 loss_train: 1.5551 acc_train: 0.9083 loss_val: 1.6656 acc_val: 0.7180 time: 0.0365s\n",
      "Epoch: 0050 loss_train: 1.5551 acc_train: 0.9083 loss_val: 1.6655 acc_val: 0.7180 time: 0.0309s\n",
      "Epoch: 0051 loss_train: 1.5550 acc_train: 0.9083 loss_val: 1.6653 acc_val: 0.7200 time: 0.0307s\n",
      "Epoch: 0052 loss_train: 1.5549 acc_train: 0.9083 loss_val: 1.6651 acc_val: 0.7240 time: 0.0284s\n",
      "Epoch: 0053 loss_train: 1.5548 acc_train: 0.9167 loss_val: 1.6648 acc_val: 0.7240 time: 0.0288s\n",
      "Epoch: 0054 loss_train: 1.5546 acc_train: 0.9167 loss_val: 1.6646 acc_val: 0.7240 time: 0.0294s\n",
      "Epoch: 0055 loss_train: 1.5544 acc_train: 0.9167 loss_val: 1.6644 acc_val: 0.7240 time: 0.0299s\n",
      "Epoch: 0056 loss_train: 1.5543 acc_train: 0.9167 loss_val: 1.6642 acc_val: 0.7240 time: 0.0331s\n",
      "Epoch: 0057 loss_train: 1.5542 acc_train: 0.9167 loss_val: 1.6641 acc_val: 0.7220 time: 0.0346s\n",
      "Epoch: 0058 loss_train: 1.5541 acc_train: 0.9167 loss_val: 1.6639 acc_val: 0.7220 time: 0.0299s\n",
      "Epoch: 0059 loss_train: 1.5539 acc_train: 0.9167 loss_val: 1.6638 acc_val: 0.7200 time: 0.0300s\n",
      "Epoch: 0060 loss_train: 1.5537 acc_train: 0.9083 loss_val: 1.6636 acc_val: 0.7200 time: 0.0298s\n",
      "Epoch: 0061 loss_train: 1.5535 acc_train: 0.9083 loss_val: 1.6634 acc_val: 0.7220 time: 0.0293s\n",
      "Epoch: 0062 loss_train: 1.5533 acc_train: 0.9083 loss_val: 1.6633 acc_val: 0.7240 time: 0.0287s\n",
      "Epoch: 0063 loss_train: 1.5531 acc_train: 0.9083 loss_val: 1.6632 acc_val: 0.7240 time: 0.0387s\n",
      "Epoch: 0064 loss_train: 1.5529 acc_train: 0.9083 loss_val: 1.6631 acc_val: 0.7240 time: 0.0316s\n",
      "Epoch: 0065 loss_train: 1.5528 acc_train: 0.9083 loss_val: 1.6630 acc_val: 0.7240 time: 0.0306s\n",
      "Epoch: 0066 loss_train: 1.5526 acc_train: 0.9083 loss_val: 1.6628 acc_val: 0.7240 time: 0.0336s\n",
      "Epoch: 0067 loss_train: 1.5525 acc_train: 0.9083 loss_val: 1.6626 acc_val: 0.7240 time: 0.0376s\n",
      "Epoch: 0068 loss_train: 1.5524 acc_train: 0.9083 loss_val: 1.6625 acc_val: 0.7260 time: 0.0294s\n",
      "Epoch: 0069 loss_train: 1.5522 acc_train: 0.9083 loss_val: 1.6624 acc_val: 0.7220 time: 0.0308s\n",
      "Epoch: 0070 loss_train: 1.5521 acc_train: 0.9083 loss_val: 1.6624 acc_val: 0.7220 time: 0.0355s\n",
      "Epoch: 0071 loss_train: 1.5521 acc_train: 0.9083 loss_val: 1.6623 acc_val: 0.7220 time: 0.0344s\n",
      "Epoch: 0072 loss_train: 1.5521 acc_train: 0.9083 loss_val: 1.6622 acc_val: 0.7220 time: 0.0384s\n",
      "Epoch: 0073 loss_train: 1.5520 acc_train: 0.9083 loss_val: 1.6620 acc_val: 0.7220 time: 0.0320s\n",
      "Epoch: 0074 loss_train: 1.5520 acc_train: 0.9167 loss_val: 1.6620 acc_val: 0.7220 time: 0.0309s\n",
      "Epoch: 0075 loss_train: 1.5519 acc_train: 0.9167 loss_val: 1.6619 acc_val: 0.7220 time: 0.0344s\n",
      "Epoch: 0076 loss_train: 1.5518 acc_train: 0.9167 loss_val: 1.6619 acc_val: 0.7220 time: 0.0419s\n",
      "Epoch: 0077 loss_train: 1.5518 acc_train: 0.9167 loss_val: 1.6619 acc_val: 0.7220 time: 0.0377s\n",
      "Epoch: 0078 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6618 acc_val: 0.7220 time: 0.0317s\n",
      "Epoch: 0079 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6618 acc_val: 0.7220 time: 0.0302s\n",
      "Epoch: 0080 loss_train: 1.5517 acc_train: 0.9083 loss_val: 1.6617 acc_val: 0.7220 time: 0.0332s\n",
      "Epoch: 0081 loss_train: 1.5516 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7200 time: 0.0422s\n",
      "Epoch: 0082 loss_train: 1.5515 acc_train: 0.9083 loss_val: 1.6616 acc_val: 0.7200 time: 0.0355s\n",
      "Epoch: 0083 loss_train: 1.5515 acc_train: 0.9083 loss_val: 1.6615 acc_val: 0.7200 time: 0.0343s\n",
      "Epoch: 0084 loss_train: 1.5514 acc_train: 0.9083 loss_val: 1.6615 acc_val: 0.7200 time: 0.0399s\n",
      "Epoch: 0085 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6614 acc_val: 0.7200 time: 0.0327s\n",
      "Epoch: 0086 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6613 acc_val: 0.7200 time: 0.0317s\n",
      "Epoch: 0087 loss_train: 1.5513 acc_train: 0.9083 loss_val: 1.6613 acc_val: 0.7200 time: 0.0346s\n",
      "Epoch: 0088 loss_train: 1.5512 acc_train: 0.9083 loss_val: 1.6612 acc_val: 0.7200 time: 0.0370s\n",
      "Epoch: 0089 loss_train: 1.5511 acc_train: 0.9083 loss_val: 1.6611 acc_val: 0.7220 time: 0.0319s\n",
      "Epoch: 0090 loss_train: 1.5511 acc_train: 0.9083 loss_val: 1.6611 acc_val: 0.7220 time: 0.0312s\n",
      "Epoch: 0091 loss_train: 1.5510 acc_train: 0.9083 loss_val: 1.6610 acc_val: 0.7220 time: 0.0295s\n",
      "Epoch: 0092 loss_train: 1.5510 acc_train: 0.9083 loss_val: 1.6610 acc_val: 0.7220 time: 0.0302s\n",
      "Epoch: 0093 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6609 acc_val: 0.7220 time: 0.0297s\n",
      "Epoch: 0094 loss_train: 1.5509 acc_train: 0.9083 loss_val: 1.6608 acc_val: 0.7220 time: 0.0377s\n",
      "Epoch: 0095 loss_train: 1.5508 acc_train: 0.9167 loss_val: 1.6608 acc_val: 0.7260 time: 0.0310s\n",
      "Epoch: 0096 loss_train: 1.5507 acc_train: 0.9167 loss_val: 1.6607 acc_val: 0.7260 time: 0.0312s\n",
      "Epoch: 0097 loss_train: 1.5507 acc_train: 0.9083 loss_val: 1.6607 acc_val: 0.7240 time: 0.0295s\n",
      "Epoch: 0098 loss_train: 1.5506 acc_train: 0.9000 loss_val: 1.6606 acc_val: 0.7240 time: 0.0312s\n",
      "Epoch: 0099 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6606 acc_val: 0.7220 time: 0.0323s\n",
      "Epoch: 0100 loss_train: 1.5506 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7260 time: 0.0310s\n",
      "Epoch: 0101 loss_train: 1.5505 acc_train: 0.9083 loss_val: 1.6605 acc_val: 0.7260 time: 0.0392s\n",
      "Epoch: 0102 loss_train: 1.5505 acc_train: 0.9083 loss_val: 1.6604 acc_val: 0.7260 time: 0.0324s\n",
      "Epoch: 0103 loss_train: 1.5504 acc_train: 0.9000 loss_val: 1.6604 acc_val: 0.7260 time: 0.0315s\n",
      "Epoch: 0104 loss_train: 1.5504 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0297s\n",
      "Epoch: 0105 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0341s\n",
      "Epoch: 0106 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.7260 time: 0.0390s\n",
      "Epoch: 0107 loss_train: 1.5503 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0316s\n",
      "Epoch: 0108 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0381s\n",
      "Epoch: 0109 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6602 acc_val: 0.7260 time: 0.0319s\n",
      "Epoch: 0110 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0311s\n",
      "Epoch: 0111 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0298s\n",
      "Epoch: 0112 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0301s\n",
      "Epoch: 0113 loss_train: 1.5502 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7260 time: 0.0298s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0114 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0396s\n",
      "Epoch: 0115 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0361s\n",
      "Epoch: 0116 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0364s\n",
      "Epoch: 0117 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0303s\n",
      "Epoch: 0118 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6601 acc_val: 0.7240 time: 0.0292s\n",
      "Epoch: 0119 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0294s\n",
      "Epoch: 0120 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0298s\n",
      "Epoch: 0121 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0392s\n",
      "Epoch: 0122 loss_train: 1.5501 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0311s\n",
      "Epoch: 0123 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0313s\n",
      "Epoch: 0124 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0296s\n",
      "Epoch: 0125 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0312s\n",
      "Epoch: 0126 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0301s\n",
      "Epoch: 0127 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0377s\n",
      "Epoch: 0128 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0314s\n",
      "Epoch: 0129 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0316s\n",
      "Epoch: 0130 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7200 time: 0.0291s\n",
      "Epoch: 0131 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7200 time: 0.0296s\n",
      "Epoch: 0132 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7200 time: 0.0292s\n",
      "Epoch: 0133 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0377s\n",
      "Epoch: 0134 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0305s\n",
      "Epoch: 0135 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0316s\n",
      "Epoch: 0136 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0304s\n",
      "Epoch: 0137 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0386s\n",
      "Epoch: 0138 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7260 time: 0.0386s\n",
      "Epoch: 0139 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0341s\n",
      "Epoch: 0140 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0418s\n",
      "Epoch: 0141 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0394s\n",
      "Epoch: 0142 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0325s\n",
      "Epoch: 0143 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0325s\n",
      "Epoch: 0144 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0302s\n",
      "Epoch: 0145 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0372s\n",
      "Epoch: 0146 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0436s\n",
      "Epoch: 0147 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0383s\n",
      "Epoch: 0148 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0408s\n",
      "Epoch: 0149 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0302s\n",
      "Epoch: 0150 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0325s\n",
      "Epoch: 0151 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0376s\n",
      "Epoch: 0152 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0297s\n",
      "Epoch: 0153 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0304s\n",
      "Epoch: 0154 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0298s\n",
      "Epoch: 0155 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0304s\n",
      "Epoch: 0156 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0301s\n",
      "Epoch: 0157 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0290s\n",
      "Epoch: 0158 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0329s\n",
      "Epoch: 0159 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0362s\n",
      "Epoch: 0160 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0352s\n",
      "Epoch: 0161 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0380s\n",
      "Epoch: 0162 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0399s\n",
      "Epoch: 0163 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0438s\n",
      "Epoch: 0164 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0404s\n",
      "Epoch: 0165 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0307s\n",
      "Epoch: 0166 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0309s\n",
      "Epoch: 0167 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0312s\n",
      "Epoch: 0168 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0315s\n",
      "Epoch: 0169 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0359s\n",
      "Epoch: 0170 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0422s\n",
      "Epoch: 0171 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0480s\n",
      "Epoch: 0172 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0397s\n",
      "Epoch: 0173 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0400s\n",
      "Epoch: 0174 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0366s\n",
      "Epoch: 0175 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0397s\n",
      "Epoch: 0176 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0458s\n",
      "Epoch: 0177 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0427s\n",
      "Epoch: 0178 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0346s\n",
      "Epoch: 0179 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0351s\n",
      "Epoch: 0180 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0385s\n",
      "Epoch: 0181 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0461s\n",
      "Epoch: 0182 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6599 acc_val: 0.7240 time: 0.0381s\n",
      "Epoch: 0183 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0370s\n",
      "Epoch: 0184 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0358s\n",
      "Epoch: 0185 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0356s\n",
      "Epoch: 0186 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0424s\n",
      "Epoch: 0187 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0398s\n",
      "Epoch: 0188 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0395s\n",
      "Epoch: 0189 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0395s\n",
      "Epoch: 0190 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0488s\n",
      "Epoch: 0191 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0432s\n",
      "Epoch: 0192 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0397s\n",
      "Epoch: 0193 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0523s\n",
      "Epoch: 0194 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0465s\n",
      "Epoch: 0195 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0527s\n",
      "Epoch: 0196 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0353s\n",
      "Epoch: 0197 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0464s\n",
      "Epoch: 0198 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0362s\n",
      "Epoch: 0199 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0423s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0200 loss_train: 1.5500 acc_train: 0.9000 loss_val: 1.6600 acc_val: 0.7240 time: 0.0393s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 7.3486s\n",
      "Best result: val_loss= 1.6599222421646118 test_loss= 1.6585291624069214 test_acc= 0.713\n",
      "Model's state_dict:\n",
      "W1.weight \t torch.Size([6, 3703])\n",
      "W1.bias \t torch.Size([6])\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False, 'params': [5105001528, 5105100120]}]\n",
      "Epoch: 0001 loss_train: 1.7917 acc_train: 0.1667 loss_val: 1.7855 acc_val: 0.3240 time: 0.0394s\n",
      "Epoch: 0002 loss_train: 1.7201 acc_train: 0.5000 loss_val: 1.7482 acc_val: 0.6200 time: 0.0352s\n",
      "Epoch: 0003 loss_train: 1.6658 acc_train: 0.9583 loss_val: 1.7237 acc_val: 0.5060 time: 0.0321s\n",
      "Epoch: 0004 loss_train: 1.6340 acc_train: 0.8417 loss_val: 1.7108 acc_val: 0.5060 time: 0.0368s\n",
      "Epoch: 0005 loss_train: 1.6123 acc_train: 0.8167 loss_val: 1.7042 acc_val: 0.6020 time: 0.0375s\n",
      "Epoch: 0006 loss_train: 1.5947 acc_train: 0.8917 loss_val: 1.7015 acc_val: 0.6900 time: 0.0458s\n",
      "Epoch: 0007 loss_train: 1.5817 acc_train: 0.9167 loss_val: 1.7005 acc_val: 0.6520 time: 0.0336s\n",
      "Epoch: 0008 loss_train: 1.5729 acc_train: 0.9333 loss_val: 1.6979 acc_val: 0.6160 time: 0.0307s\n",
      "Epoch: 0009 loss_train: 1.5673 acc_train: 0.9250 loss_val: 1.6931 acc_val: 0.6620 time: 0.0301s\n",
      "Epoch: 0010 loss_train: 1.5639 acc_train: 0.9333 loss_val: 1.6874 acc_val: 0.7120 time: 0.0321s\n",
      "Epoch: 0011 loss_train: 1.5627 acc_train: 0.9333 loss_val: 1.6824 acc_val: 0.7260 time: 0.0302s\n",
      "Epoch: 0012 loss_train: 1.5633 acc_train: 0.9083 loss_val: 1.6791 acc_val: 0.7140 time: 0.0342s\n",
      "Epoch: 0013 loss_train: 1.5645 acc_train: 0.8917 loss_val: 1.6776 acc_val: 0.7060 time: 0.0363s\n",
      "Epoch: 0014 loss_train: 1.5652 acc_train: 0.8833 loss_val: 1.6775 acc_val: 0.7020 time: 0.0316s\n",
      "Epoch: 0015 loss_train: 1.5650 acc_train: 0.9000 loss_val: 1.6782 acc_val: 0.6880 time: 0.0343s\n",
      "Epoch: 0016 loss_train: 1.5640 acc_train: 0.9083 loss_val: 1.6790 acc_val: 0.6880 time: 0.0388s\n",
      "Epoch: 0017 loss_train: 1.5629 acc_train: 0.9083 loss_val: 1.6790 acc_val: 0.6860 time: 0.0363s\n",
      "Epoch: 0018 loss_train: 1.5619 acc_train: 0.9000 loss_val: 1.6779 acc_val: 0.7020 time: 0.0439s\n",
      "Epoch: 0019 loss_train: 1.5613 acc_train: 0.9167 loss_val: 1.6762 acc_val: 0.7100 time: 0.0321s\n",
      "Epoch: 0020 loss_train: 1.5613 acc_train: 0.9250 loss_val: 1.6743 acc_val: 0.7180 time: 0.0305s\n",
      "Epoch: 0021 loss_train: 1.5620 acc_train: 0.9167 loss_val: 1.6729 acc_val: 0.7300 time: 0.0319s\n",
      "Epoch: 0022 loss_train: 1.5628 acc_train: 0.9083 loss_val: 1.6722 acc_val: 0.7220 time: 0.0297s\n",
      "Epoch: 0023 loss_train: 1.5634 acc_train: 0.9083 loss_val: 1.6721 acc_val: 0.7160 time: 0.0290s\n",
      "Epoch: 0024 loss_train: 1.5633 acc_train: 0.9083 loss_val: 1.6723 acc_val: 0.7080 time: 0.0314s\n",
      "Epoch: 0025 loss_train: 1.5627 acc_train: 0.9167 loss_val: 1.6724 acc_val: 0.7020 time: 0.0383s\n",
      "Epoch: 0026 loss_train: 1.5616 acc_train: 0.9083 loss_val: 1.6720 acc_val: 0.7060 time: 0.0305s\n",
      "Epoch: 0027 loss_train: 1.5605 acc_train: 0.9083 loss_val: 1.6711 acc_val: 0.7100 time: 0.0330s\n",
      "Epoch: 0028 loss_train: 1.5595 acc_train: 0.9083 loss_val: 1.6700 acc_val: 0.7200 time: 0.0320s\n",
      "Epoch: 0029 loss_train: 1.5590 acc_train: 0.9083 loss_val: 1.6690 acc_val: 0.7220 time: 0.0306s\n",
      "Epoch: 0030 loss_train: 1.5589 acc_train: 0.9167 loss_val: 1.6685 acc_val: 0.7320 time: 0.0316s\n",
      "Epoch: 0031 loss_train: 1.5591 acc_train: 0.9167 loss_val: 1.6684 acc_val: 0.7260 time: 0.0326s\n",
      "Epoch: 0032 loss_train: 1.5593 acc_train: 0.9167 loss_val: 1.6686 acc_val: 0.7080 time: 0.0418s\n",
      "Epoch: 0033 loss_train: 1.5593 acc_train: 0.9083 loss_val: 1.6688 acc_val: 0.7060 time: 0.0330s\n",
      "Epoch: 0034 loss_train: 1.5591 acc_train: 0.9083 loss_val: 1.6688 acc_val: 0.7060 time: 0.0304s\n",
      "Epoch: 0035 loss_train: 1.5587 acc_train: 0.9083 loss_val: 1.6684 acc_val: 0.7100 time: 0.0305s\n",
      "Epoch: 0036 loss_train: 1.5581 acc_train: 0.9083 loss_val: 1.6679 acc_val: 0.7200 time: 0.0322s\n",
      "Epoch: 0037 loss_train: 1.5576 acc_train: 0.9083 loss_val: 1.6673 acc_val: 0.7260 time: 0.0299s\n",
      "Epoch: 0038 loss_train: 1.5573 acc_train: 0.9000 loss_val: 1.6670 acc_val: 0.7260 time: 0.0297s\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(10):\n",
    "    acc.append(get_acc(adj, feats, labels, idx_train, idx_val, idx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = np.array(acc)\n",
    "mean = acc.mean()\n",
    "var = acc.var()\n",
    "print(acc)\n",
    "print(mean,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.sqrt(var)\n",
    "print(mean*100,std*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
