{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cpnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a9afc2f12bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google_Drive/Research/GraphConvolutionalNetworks/GNN_code_self 2/src/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from plots import plot_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcpnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cpnet'"
     ]
    }
   ],
   "source": [
    "from src.get_data import load_benchmark\n",
    "from src.normalization import get_adj_feats\n",
    "from src.args import get_args\n",
    "from src.models import get_model\n",
    "from src.utils import accuracy\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "finish load data\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "\n",
    "# all tensor, dense\n",
    "dataset_name = 'cora'\n",
    "# dataset_name = input('input dataset name: cora/citeseer/pubmed/...')\n",
    "\n",
    "\n",
    "adj, feats, labels, idx_train, idx_val, idx_test = load_benchmark(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get args\n",
    "\n",
    "# model_name = input('choose model: GCN/SGC/GFNN/GFN/AGNN/GIN/...')\n",
    "model_name = 'GCN'\n",
    "args = get_args(model_opt = model_name, dataset = dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for GCN, return sym_norm(A) and raw feats\n"
     ]
    }
   ],
   "source": [
    "# get input for model\n",
    "\n",
    "adj, feats = get_adj_feats(adj = adj, feats = feats, model_opt = model_name, degree = args.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing\n",
    "\n",
    "nb_class = (torch.max(labels) + 1).numpy()\n",
    "Y_onehot =  torch.zeros(labels.shape[0], nb_class).scatter_(1, labels.unsqueeze(-1), 1)\n",
    "\n",
    "nb_each_class_train = torch.sum(Y_onehot[idx_train], dim = 0)\n",
    "nb_each_class_inv_train = torch.tensor(np.power(nb_each_class_train.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_train = torch.diag(nb_each_class_inv_train)\n",
    "\n",
    "nb_each_class_val = torch.sum(Y_onehot[idx_val], dim = 0)\n",
    "nb_each_class_inv_val = torch.tensor(np.power(nb_each_class_val.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_val = torch.diag(nb_each_class_inv_val)\n",
    "\n",
    "nb_each_class_test = torch.sum(Y_onehot[idx_test], dim = 0)\n",
    "nb_each_class_inv_test = torch.tensor(np.power(nb_each_class_test.numpy(), -1).flatten())\n",
    "nb_each_class_inv_mat_test = torch.diag(nb_each_class_inv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "gc1.weight \t torch.Size([1433, 16])\n",
      "gc1.bias \t torch.Size([16])\n",
      "gc2.weight \t torch.Size([16, 7])\n",
      "gc2.bias \t torch.Size([7])\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0005, 'amsgrad': False, 'params': [0, 1, 2, 3]}]\n",
      "gc1.weight Parameter containing:\n",
      "tensor([[ 0.2102, -0.2396,  0.1495,  ...,  0.0294,  0.1798, -0.0105],\n",
      "        [-0.1021,  0.1460,  0.0942,  ...,  0.0229,  0.0329,  0.1903],\n",
      "        [ 0.0431, -0.0676,  0.0863,  ..., -0.0159, -0.1731,  0.1386],\n",
      "        ...,\n",
      "        [ 0.1948,  0.0915, -0.2434,  ..., -0.0467, -0.2098,  0.1553],\n",
      "        [-0.0135, -0.2178,  0.0473,  ...,  0.2128, -0.0957,  0.0534],\n",
      "        [-0.1084, -0.0559, -0.1729,  ...,  0.1303,  0.1701, -0.0910]],\n",
      "       requires_grad=True)\n",
      "gc1.bias Parameter containing:\n",
      "tensor([-0.1308,  0.1932,  0.1251,  0.1600, -0.0565,  0.1765, -0.0049, -0.1707,\n",
      "         0.0613,  0.1943, -0.1136,  0.0755, -0.2350,  0.1963, -0.2077, -0.1340],\n",
      "       requires_grad=True)\n",
      "gc2.weight Parameter containing:\n",
      "tensor([[ 0.1102,  0.1270,  0.2409, -0.2320, -0.2879, -0.2515, -0.2182],\n",
      "        [-0.2431,  0.1979,  0.1026, -0.3616, -0.2955,  0.2493,  0.3282],\n",
      "        [-0.0043, -0.0499,  0.3753,  0.1623,  0.0451,  0.1887,  0.0282],\n",
      "        [-0.1032,  0.3210, -0.3517,  0.1306, -0.3406, -0.0260,  0.3715],\n",
      "        [-0.2187, -0.3673, -0.1652, -0.0934,  0.2504, -0.3493,  0.1963],\n",
      "        [-0.1776, -0.0646, -0.1892,  0.3308, -0.2556, -0.2230,  0.0090],\n",
      "        [ 0.2241, -0.3079, -0.3437, -0.2377,  0.0049,  0.3010, -0.0836],\n",
      "        [ 0.2741,  0.2859,  0.0204, -0.1620,  0.2269,  0.0030,  0.3134],\n",
      "        [ 0.2685, -0.1680, -0.3616, -0.1135,  0.3088,  0.0409, -0.3182],\n",
      "        [ 0.3473,  0.2223, -0.2698, -0.0772,  0.1932,  0.2805, -0.1518],\n",
      "        [-0.1407, -0.1738, -0.0761,  0.0653, -0.0253, -0.1708,  0.2871],\n",
      "        [-0.2476,  0.0556, -0.3506,  0.2804, -0.2453, -0.1113, -0.0804],\n",
      "        [ 0.1055, -0.1438,  0.2588, -0.3200, -0.0564, -0.1089,  0.1896],\n",
      "        [ 0.2282,  0.2582,  0.3023, -0.1626, -0.1777, -0.1811, -0.3535],\n",
      "        [ 0.0929,  0.0357, -0.2120,  0.0125,  0.2397, -0.3383, -0.2385],\n",
      "        [ 0.0652, -0.0379, -0.0774,  0.1249,  0.3411,  0.3255, -0.3206]],\n",
      "       requires_grad=True)\n",
      "gc2.bias Parameter containing:\n",
      "tensor([-0.3250, -0.1161, -0.2615, -0.1668,  0.0237, -0.1054,  0.1965],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# get model\n",
    "model = get_model(model_opt = model_name, nfeat = feats.size(1), \\\n",
    "                  nclass = labels.max().item()+1, nhid = args.hidden, \\\n",
    "                  dropout = args.dropout, cuda = args.cuda, \\\n",
    "                  dataset = dataset_name, degree = args.degree)\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "# if args.cuda:\n",
    "#     if model_name!='AGNN' and model_name!='GIN':\n",
    "#         model.cuda()\n",
    "#         feats = feats.cuda()\n",
    "#         adj = adj.cuda()\n",
    "#         labels = labels.cuda()\n",
    "#         idx_train = idx_train.cuda()\n",
    "#         idx_val = idx_val.cuda()\n",
    "#         idx_test = idx_test.cuda()\n",
    "    \n",
    "    \n",
    "# Print model's state_dict    \n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor,\"\\t\",model.state_dict()[param_tensor].size()) \n",
    "print(\"optimizer's state_dict:\")\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name,\"\\t\",optimizer.state_dict()[var_name])\n",
    "    \n",
    "# Print parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delt = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, fp1, fp2 = model(feats, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output, fp1, fp2 = model(feats, adj)\n",
    "    \n",
    "    CE_loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    loss_val = CE_loss_val\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    CE_loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    loss_test = CE_loss_test\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'loss_test: {:.4f}'.format(loss_test.item()),\n",
    "          'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    return epoch+1, loss_train.item(), acc_train.item(), loss_val.item(), \\\n",
    "            acc_val.item(), loss_test.item(), acc_test.item(), time.time() - t, \\\n",
    "            output, fp1, fp2\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.1928 acc_train: 1.0000 loss_val: 0.7616 acc_val: 0.7960 loss_test: 0.7308 acc_test: 0.8160 time: 0.0330s\n",
      "-------------------------------------------------\n",
      "Epoch: 0002 loss_train: 0.1921 acc_train: 1.0000 loss_val: 0.7620 acc_val: 0.7940 loss_test: 0.7310 acc_test: 0.8150 time: 0.0427s\n",
      "-------------------------------------------------\n",
      "Epoch: 0003 loss_train: 0.1913 acc_train: 1.0000 loss_val: 0.7613 acc_val: 0.7940 loss_test: 0.7305 acc_test: 0.8130 time: 0.0231s\n",
      "-------------------------------------------------\n",
      "Epoch: 0004 loss_train: 0.1905 acc_train: 1.0000 loss_val: 0.7610 acc_val: 0.7940 loss_test: 0.7302 acc_test: 0.8130 time: 0.0275s\n",
      "-------------------------------------------------\n",
      "Epoch: 0005 loss_train: 0.1897 acc_train: 1.0000 loss_val: 0.7598 acc_val: 0.7940 loss_test: 0.7291 acc_test: 0.8150 time: 0.0197s\n",
      "-------------------------------------------------\n",
      "Epoch: 0006 loss_train: 0.1890 acc_train: 1.0000 loss_val: 0.7594 acc_val: 0.7940 loss_test: 0.7287 acc_test: 0.8150 time: 0.0229s\n",
      "-------------------------------------------------\n",
      "Epoch: 0007 loss_train: 0.1882 acc_train: 1.0000 loss_val: 0.7591 acc_val: 0.7940 loss_test: 0.7284 acc_test: 0.8130 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0008 loss_train: 0.1875 acc_train: 1.0000 loss_val: 0.7584 acc_val: 0.7940 loss_test: 0.7278 acc_test: 0.8150 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0009 loss_train: 0.1868 acc_train: 1.0000 loss_val: 0.7580 acc_val: 0.7940 loss_test: 0.7274 acc_test: 0.8130 time: 0.0159s\n",
      "-------------------------------------------------\n",
      "Epoch: 0010 loss_train: 0.1861 acc_train: 1.0000 loss_val: 0.7567 acc_val: 0.7940 loss_test: 0.7263 acc_test: 0.8130 time: 0.0217s\n",
      "-------------------------------------------------\n",
      "Epoch: 0011 loss_train: 0.1854 acc_train: 1.0000 loss_val: 0.7568 acc_val: 0.7940 loss_test: 0.7263 acc_test: 0.8150 time: 0.0164s\n",
      "-------------------------------------------------\n",
      "Epoch: 0012 loss_train: 0.1847 acc_train: 1.0000 loss_val: 0.7561 acc_val: 0.7940 loss_test: 0.7258 acc_test: 0.8140 time: 0.0188s\n",
      "-------------------------------------------------\n",
      "Epoch: 0013 loss_train: 0.1840 acc_train: 1.0000 loss_val: 0.7567 acc_val: 0.7940 loss_test: 0.7261 acc_test: 0.8140 time: 0.0162s\n",
      "-------------------------------------------------\n",
      "Epoch: 0014 loss_train: 0.1834 acc_train: 1.0000 loss_val: 0.7540 acc_val: 0.7960 loss_test: 0.7240 acc_test: 0.8140 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0015 loss_train: 0.1827 acc_train: 1.0000 loss_val: 0.7548 acc_val: 0.7940 loss_test: 0.7245 acc_test: 0.8140 time: 0.0181s\n",
      "-------------------------------------------------\n",
      "Epoch: 0016 loss_train: 0.1820 acc_train: 1.0000 loss_val: 0.7541 acc_val: 0.7940 loss_test: 0.7240 acc_test: 0.8140 time: 0.0176s\n",
      "-------------------------------------------------\n",
      "Epoch: 0017 loss_train: 0.1814 acc_train: 1.0000 loss_val: 0.7541 acc_val: 0.7940 loss_test: 0.7239 acc_test: 0.8150 time: 0.0161s\n",
      "-------------------------------------------------\n",
      "Epoch: 0018 loss_train: 0.1808 acc_train: 1.0000 loss_val: 0.7526 acc_val: 0.7940 loss_test: 0.7227 acc_test: 0.8150 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0019 loss_train: 0.1802 acc_train: 1.0000 loss_val: 0.7526 acc_val: 0.7940 loss_test: 0.7226 acc_test: 0.8140 time: 0.0164s\n",
      "-------------------------------------------------\n",
      "Epoch: 0020 loss_train: 0.1795 acc_train: 1.0000 loss_val: 0.7527 acc_val: 0.7940 loss_test: 0.7227 acc_test: 0.8150 time: 0.0159s\n",
      "-------------------------------------------------\n",
      "Epoch: 0021 loss_train: 0.1789 acc_train: 1.0000 loss_val: 0.7515 acc_val: 0.7940 loss_test: 0.7217 acc_test: 0.8140 time: 0.0174s\n",
      "-------------------------------------------------\n",
      "Epoch: 0022 loss_train: 0.1783 acc_train: 1.0000 loss_val: 0.7520 acc_val: 0.7940 loss_test: 0.7219 acc_test: 0.8140 time: 0.0171s\n",
      "-------------------------------------------------\n",
      "Epoch: 0023 loss_train: 0.1777 acc_train: 1.0000 loss_val: 0.7496 acc_val: 0.7960 loss_test: 0.7201 acc_test: 0.8140 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0024 loss_train: 0.1771 acc_train: 1.0000 loss_val: 0.7512 acc_val: 0.7940 loss_test: 0.7212 acc_test: 0.8150 time: 0.0181s\n",
      "-------------------------------------------------\n",
      "Epoch: 0025 loss_train: 0.1766 acc_train: 1.0000 loss_val: 0.7492 acc_val: 0.7940 loss_test: 0.7197 acc_test: 0.8150 time: 0.0198s\n",
      "-------------------------------------------------\n",
      "Epoch: 0026 loss_train: 0.1760 acc_train: 1.0000 loss_val: 0.7503 acc_val: 0.7940 loss_test: 0.7204 acc_test: 0.8150 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0027 loss_train: 0.1754 acc_train: 1.0000 loss_val: 0.7487 acc_val: 0.7940 loss_test: 0.7192 acc_test: 0.8140 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0028 loss_train: 0.1749 acc_train: 1.0000 loss_val: 0.7484 acc_val: 0.7940 loss_test: 0.7189 acc_test: 0.8150 time: 0.0167s\n",
      "-------------------------------------------------\n",
      "Epoch: 0029 loss_train: 0.1743 acc_train: 1.0000 loss_val: 0.7490 acc_val: 0.7940 loss_test: 0.7193 acc_test: 0.8150 time: 0.0185s\n",
      "-------------------------------------------------\n",
      "Epoch: 0030 loss_train: 0.1738 acc_train: 1.0000 loss_val: 0.7472 acc_val: 0.7940 loss_test: 0.7179 acc_test: 0.8160 time: 0.0168s\n",
      "-------------------------------------------------\n",
      "Epoch: 0031 loss_train: 0.1732 acc_train: 1.0000 loss_val: 0.7477 acc_val: 0.7940 loss_test: 0.7181 acc_test: 0.8150 time: 0.0182s\n",
      "-------------------------------------------------\n",
      "Epoch: 0032 loss_train: 0.1727 acc_train: 1.0000 loss_val: 0.7462 acc_val: 0.7960 loss_test: 0.7169 acc_test: 0.8150 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0033 loss_train: 0.1722 acc_train: 1.0000 loss_val: 0.7473 acc_val: 0.7940 loss_test: 0.7178 acc_test: 0.8140 time: 0.0236s\n",
      "-------------------------------------------------\n",
      "Epoch: 0034 loss_train: 0.1717 acc_train: 1.0000 loss_val: 0.7462 acc_val: 0.7940 loss_test: 0.7169 acc_test: 0.8160 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0035 loss_train: 0.1712 acc_train: 1.0000 loss_val: 0.7459 acc_val: 0.7940 loss_test: 0.7165 acc_test: 0.8160 time: 0.0201s\n",
      "-------------------------------------------------\n",
      "Epoch: 0036 loss_train: 0.1707 acc_train: 1.0000 loss_val: 0.7452 acc_val: 0.7940 loss_test: 0.7160 acc_test: 0.8140 time: 0.0171s\n",
      "-------------------------------------------------\n",
      "Epoch: 0037 loss_train: 0.1702 acc_train: 1.0000 loss_val: 0.7454 acc_val: 0.7940 loss_test: 0.7161 acc_test: 0.8150 time: 0.0174s\n",
      "-------------------------------------------------\n",
      "Epoch: 0038 loss_train: 0.1697 acc_train: 1.0000 loss_val: 0.7445 acc_val: 0.7940 loss_test: 0.7154 acc_test: 0.8160 time: 0.0173s\n",
      "-------------------------------------------------\n",
      "Epoch: 0039 loss_train: 0.1692 acc_train: 1.0000 loss_val: 0.7448 acc_val: 0.7940 loss_test: 0.7155 acc_test: 0.8150 time: 0.0173s\n",
      "-------------------------------------------------\n",
      "Epoch: 0040 loss_train: 0.1687 acc_train: 1.0000 loss_val: 0.7437 acc_val: 0.7940 loss_test: 0.7147 acc_test: 0.8150 time: 0.0182s\n",
      "-------------------------------------------------\n",
      "Epoch: 0041 loss_train: 0.1682 acc_train: 1.0000 loss_val: 0.7436 acc_val: 0.7940 loss_test: 0.7145 acc_test: 0.8150 time: 0.0170s\n",
      "-------------------------------------------------\n",
      "Epoch: 0042 loss_train: 0.1678 acc_train: 1.0000 loss_val: 0.7432 acc_val: 0.7940 loss_test: 0.7142 acc_test: 0.8130 time: 0.0189s\n",
      "-------------------------------------------------\n",
      "Epoch: 0043 loss_train: 0.1673 acc_train: 1.0000 loss_val: 0.7439 acc_val: 0.7920 loss_test: 0.7147 acc_test: 0.8130 time: 0.0191s\n",
      "-------------------------------------------------\n",
      "Epoch: 0044 loss_train: 0.1669 acc_train: 1.0000 loss_val: 0.7424 acc_val: 0.7940 loss_test: 0.7136 acc_test: 0.8120 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0045 loss_train: 0.1664 acc_train: 1.0000 loss_val: 0.7426 acc_val: 0.7940 loss_test: 0.7135 acc_test: 0.8140 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0046 loss_train: 0.1660 acc_train: 1.0000 loss_val: 0.7414 acc_val: 0.7960 loss_test: 0.7126 acc_test: 0.8120 time: 0.0176s\n",
      "-------------------------------------------------\n",
      "Epoch: 0047 loss_train: 0.1655 acc_train: 1.0000 loss_val: 0.7418 acc_val: 0.7940 loss_test: 0.7128 acc_test: 0.8140 time: 0.0181s\n",
      "-------------------------------------------------\n",
      "Epoch: 0048 loss_train: 0.1651 acc_train: 1.0000 loss_val: 0.7420 acc_val: 0.7920 loss_test: 0.7130 acc_test: 0.8130 time: 0.0167s\n",
      "-------------------------------------------------\n",
      "Epoch: 0049 loss_train: 0.1647 acc_train: 1.0000 loss_val: 0.7412 acc_val: 0.7920 loss_test: 0.7123 acc_test: 0.8130 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0050 loss_train: 0.1642 acc_train: 1.0000 loss_val: 0.7409 acc_val: 0.7940 loss_test: 0.7120 acc_test: 0.8130 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0051 loss_train: 0.1638 acc_train: 1.0000 loss_val: 0.7398 acc_val: 0.7960 loss_test: 0.7112 acc_test: 0.8130 time: 0.0155s\n",
      "-------------------------------------------------\n",
      "Epoch: 0052 loss_train: 0.1634 acc_train: 1.0000 loss_val: 0.7415 acc_val: 0.7920 loss_test: 0.7123 acc_test: 0.8120 time: 0.0187s\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0053 loss_train: 0.1630 acc_train: 1.0000 loss_val: 0.7387 acc_val: 0.7960 loss_test: 0.7103 acc_test: 0.8110 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0054 loss_train: 0.1626 acc_train: 1.0000 loss_val: 0.7411 acc_val: 0.7920 loss_test: 0.7119 acc_test: 0.8130 time: 0.0206s\n",
      "-------------------------------------------------\n",
      "Epoch: 0055 loss_train: 0.1622 acc_train: 1.0000 loss_val: 0.7375 acc_val: 0.7960 loss_test: 0.7094 acc_test: 0.8120 time: 0.0162s\n",
      "-------------------------------------------------\n",
      "Epoch: 0056 loss_train: 0.1618 acc_train: 1.0000 loss_val: 0.7409 acc_val: 0.7920 loss_test: 0.7117 acc_test: 0.8120 time: 0.0190s\n",
      "-------------------------------------------------\n",
      "Epoch: 0057 loss_train: 0.1614 acc_train: 1.0000 loss_val: 0.7378 acc_val: 0.7960 loss_test: 0.7095 acc_test: 0.8120 time: 0.0164s\n",
      "-------------------------------------------------\n",
      "Epoch: 0058 loss_train: 0.1610 acc_train: 1.0000 loss_val: 0.7388 acc_val: 0.7920 loss_test: 0.7101 acc_test: 0.8110 time: 0.0189s\n",
      "-------------------------------------------------\n",
      "Epoch: 0059 loss_train: 0.1606 acc_train: 1.0000 loss_val: 0.7389 acc_val: 0.7920 loss_test: 0.7101 acc_test: 0.8100 time: 0.0178s\n",
      "-------------------------------------------------\n",
      "Epoch: 0060 loss_train: 0.1602 acc_train: 1.0000 loss_val: 0.7372 acc_val: 0.7940 loss_test: 0.7089 acc_test: 0.8120 time: 0.0159s\n",
      "-------------------------------------------------\n",
      "Epoch: 0061 loss_train: 0.1598 acc_train: 1.0000 loss_val: 0.7386 acc_val: 0.7920 loss_test: 0.7098 acc_test: 0.8120 time: 0.0180s\n",
      "-------------------------------------------------\n",
      "Epoch: 0062 loss_train: 0.1595 acc_train: 1.0000 loss_val: 0.7363 acc_val: 0.7960 loss_test: 0.7081 acc_test: 0.8110 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0063 loss_train: 0.1591 acc_train: 1.0000 loss_val: 0.7378 acc_val: 0.7920 loss_test: 0.7091 acc_test: 0.8110 time: 0.0209s\n",
      "-------------------------------------------------\n",
      "Epoch: 0064 loss_train: 0.1587 acc_train: 1.0000 loss_val: 0.7376 acc_val: 0.7920 loss_test: 0.7089 acc_test: 0.8110 time: 0.0178s\n",
      "-------------------------------------------------\n",
      "Epoch: 0065 loss_train: 0.1584 acc_train: 1.0000 loss_val: 0.7357 acc_val: 0.7940 loss_test: 0.7075 acc_test: 0.8120 time: 0.0186s\n",
      "-------------------------------------------------\n",
      "Epoch: 0066 loss_train: 0.1580 acc_train: 1.0000 loss_val: 0.7373 acc_val: 0.7940 loss_test: 0.7086 acc_test: 0.8120 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0067 loss_train: 0.1577 acc_train: 1.0000 loss_val: 0.7354 acc_val: 0.7940 loss_test: 0.7073 acc_test: 0.8120 time: 0.0190s\n",
      "-------------------------------------------------\n",
      "Epoch: 0068 loss_train: 0.1573 acc_train: 1.0000 loss_val: 0.7368 acc_val: 0.7920 loss_test: 0.7082 acc_test: 0.8120 time: 0.0160s\n",
      "-------------------------------------------------\n",
      "Epoch: 0069 loss_train: 0.1570 acc_train: 1.0000 loss_val: 0.7356 acc_val: 0.7920 loss_test: 0.7072 acc_test: 0.8120 time: 0.0176s\n",
      "-------------------------------------------------\n",
      "Epoch: 0070 loss_train: 0.1566 acc_train: 1.0000 loss_val: 0.7346 acc_val: 0.7940 loss_test: 0.7064 acc_test: 0.8110 time: 0.0167s\n",
      "-------------------------------------------------\n",
      "Epoch: 0071 loss_train: 0.1563 acc_train: 1.0000 loss_val: 0.7361 acc_val: 0.7920 loss_test: 0.7075 acc_test: 0.8100 time: 0.0184s\n",
      "-------------------------------------------------\n",
      "Epoch: 0072 loss_train: 0.1560 acc_train: 1.0000 loss_val: 0.7347 acc_val: 0.7920 loss_test: 0.7066 acc_test: 0.8120 time: 0.0189s\n",
      "-------------------------------------------------\n",
      "Epoch: 0073 loss_train: 0.1556 acc_train: 1.0000 loss_val: 0.7357 acc_val: 0.7920 loss_test: 0.7072 acc_test: 0.8110 time: 0.0172s\n",
      "-------------------------------------------------\n",
      "Epoch: 0074 loss_train: 0.1553 acc_train: 1.0000 loss_val: 0.7336 acc_val: 0.7940 loss_test: 0.7055 acc_test: 0.8120 time: 0.0185s\n",
      "-------------------------------------------------\n",
      "Epoch: 0075 loss_train: 0.1550 acc_train: 1.0000 loss_val: 0.7337 acc_val: 0.7940 loss_test: 0.7056 acc_test: 0.8110 time: 0.0194s\n",
      "-------------------------------------------------\n",
      "Epoch: 0076 loss_train: 0.1546 acc_train: 1.0000 loss_val: 0.7351 acc_val: 0.7920 loss_test: 0.7066 acc_test: 0.8110 time: 0.0204s\n",
      "-------------------------------------------------\n",
      "Epoch: 0077 loss_train: 0.1543 acc_train: 1.0000 loss_val: 0.7336 acc_val: 0.7920 loss_test: 0.7056 acc_test: 0.8120 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0078 loss_train: 0.1540 acc_train: 1.0000 loss_val: 0.7337 acc_val: 0.7940 loss_test: 0.7054 acc_test: 0.8130 time: 0.0193s\n",
      "-------------------------------------------------\n",
      "Epoch: 0079 loss_train: 0.1537 acc_train: 1.0000 loss_val: 0.7328 acc_val: 0.7940 loss_test: 0.7048 acc_test: 0.8110 time: 0.0177s\n",
      "-------------------------------------------------\n",
      "Epoch: 0080 loss_train: 0.1534 acc_train: 1.0000 loss_val: 0.7334 acc_val: 0.7920 loss_test: 0.7052 acc_test: 0.8110 time: 0.0173s\n",
      "-------------------------------------------------\n",
      "Epoch: 0081 loss_train: 0.1531 acc_train: 1.0000 loss_val: 0.7338 acc_val: 0.7920 loss_test: 0.7056 acc_test: 0.8110 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0082 loss_train: 0.1528 acc_train: 1.0000 loss_val: 0.7324 acc_val: 0.7920 loss_test: 0.7045 acc_test: 0.8110 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0083 loss_train: 0.1525 acc_train: 1.0000 loss_val: 0.7321 acc_val: 0.7940 loss_test: 0.7041 acc_test: 0.8130 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0084 loss_train: 0.1522 acc_train: 1.0000 loss_val: 0.7321 acc_val: 0.7940 loss_test: 0.7041 acc_test: 0.8110 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0085 loss_train: 0.1519 acc_train: 1.0000 loss_val: 0.7323 acc_val: 0.7920 loss_test: 0.7043 acc_test: 0.8110 time: 0.0157s\n",
      "-------------------------------------------------\n",
      "Epoch: 0086 loss_train: 0.1516 acc_train: 1.0000 loss_val: 0.7325 acc_val: 0.7920 loss_test: 0.7043 acc_test: 0.8110 time: 0.0197s\n",
      "-------------------------------------------------\n",
      "Epoch: 0087 loss_train: 0.1513 acc_train: 1.0000 loss_val: 0.7311 acc_val: 0.7920 loss_test: 0.7033 acc_test: 0.8110 time: 0.0163s\n",
      "-------------------------------------------------\n",
      "Epoch: 0088 loss_train: 0.1510 acc_train: 1.0000 loss_val: 0.7317 acc_val: 0.7920 loss_test: 0.7036 acc_test: 0.8110 time: 0.0177s\n",
      "-------------------------------------------------\n",
      "Epoch: 0089 loss_train: 0.1508 acc_train: 1.0000 loss_val: 0.7314 acc_val: 0.7900 loss_test: 0.7035 acc_test: 0.8100 time: 0.0166s\n",
      "-------------------------------------------------\n",
      "Epoch: 0090 loss_train: 0.1505 acc_train: 1.0000 loss_val: 0.7314 acc_val: 0.7920 loss_test: 0.7035 acc_test: 0.8110 time: 0.0156s\n",
      "-------------------------------------------------\n",
      "Epoch: 0091 loss_train: 0.1502 acc_train: 1.0000 loss_val: 0.7312 acc_val: 0.7920 loss_test: 0.7032 acc_test: 0.8110 time: 0.0165s\n",
      "-------------------------------------------------\n",
      "Epoch: 0092 loss_train: 0.1499 acc_train: 1.0000 loss_val: 0.7297 acc_val: 0.7920 loss_test: 0.7021 acc_test: 0.8140 time: 0.0164s\n",
      "-------------------------------------------------\n",
      "Epoch: 0093 loss_train: 0.1496 acc_train: 1.0000 loss_val: 0.7314 acc_val: 0.7920 loss_test: 0.7032 acc_test: 0.8100 time: 0.0186s\n",
      "-------------------------------------------------\n",
      "Epoch: 0094 loss_train: 0.1494 acc_train: 1.0000 loss_val: 0.7299 acc_val: 0.7920 loss_test: 0.7023 acc_test: 0.8120 time: 0.0159s\n",
      "-------------------------------------------------\n",
      "Epoch: 0095 loss_train: 0.1491 acc_train: 1.0000 loss_val: 0.7304 acc_val: 0.7940 loss_test: 0.7025 acc_test: 0.8130 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0096 loss_train: 0.1488 acc_train: 1.0000 loss_val: 0.7299 acc_val: 0.7920 loss_test: 0.7021 acc_test: 0.8090 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0097 loss_train: 0.1486 acc_train: 1.0000 loss_val: 0.7302 acc_val: 0.7920 loss_test: 0.7023 acc_test: 0.8090 time: 0.0198s\n",
      "-------------------------------------------------\n",
      "Epoch: 0098 loss_train: 0.1483 acc_train: 1.0000 loss_val: 0.7298 acc_val: 0.7900 loss_test: 0.7020 acc_test: 0.8130 time: 0.0184s\n",
      "-------------------------------------------------\n",
      "Epoch: 0099 loss_train: 0.1481 acc_train: 1.0000 loss_val: 0.7290 acc_val: 0.7920 loss_test: 0.7014 acc_test: 0.8090 time: 0.0215s\n",
      "-------------------------------------------------\n",
      "Epoch: 0100 loss_train: 0.1478 acc_train: 1.0000 loss_val: 0.7298 acc_val: 0.7920 loss_test: 0.7018 acc_test: 0.8090 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0101 loss_train: 0.1475 acc_train: 1.0000 loss_val: 0.7283 acc_val: 0.7920 loss_test: 0.7008 acc_test: 0.8140 time: 0.0209s\n",
      "-------------------------------------------------\n",
      "Epoch: 0102 loss_train: 0.1473 acc_train: 1.0000 loss_val: 0.7302 acc_val: 0.7920 loss_test: 0.7021 acc_test: 0.8110 time: 0.0170s\n",
      "-------------------------------------------------\n",
      "Epoch: 0103 loss_train: 0.1470 acc_train: 1.0000 loss_val: 0.7280 acc_val: 0.7920 loss_test: 0.7006 acc_test: 0.8110 time: 0.0200s\n",
      "-------------------------------------------------\n",
      "Epoch: 0104 loss_train: 0.1468 acc_train: 1.0000 loss_val: 0.7283 acc_val: 0.7920 loss_test: 0.7007 acc_test: 0.8130 time: 0.0170s\n",
      "-------------------------------------------------\n",
      "Epoch: 0105 loss_train: 0.1466 acc_train: 1.0000 loss_val: 0.7281 acc_val: 0.7920 loss_test: 0.7006 acc_test: 0.8090 time: 0.0192s\n",
      "-------------------------------------------------\n",
      "Epoch: 0106 loss_train: 0.1463 acc_train: 1.0000 loss_val: 0.7289 acc_val: 0.7920 loss_test: 0.7012 acc_test: 0.8110 time: 0.0171s\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0107 loss_train: 0.1461 acc_train: 1.0000 loss_val: 0.7287 acc_val: 0.7900 loss_test: 0.7009 acc_test: 0.8120 time: 0.0175s\n",
      "-------------------------------------------------\n",
      "Epoch: 0108 loss_train: 0.1459 acc_train: 1.0000 loss_val: 0.7270 acc_val: 0.7920 loss_test: 0.6996 acc_test: 0.8130 time: 0.0201s\n",
      "-------------------------------------------------\n",
      "Epoch: 0109 loss_train: 0.1456 acc_train: 1.0000 loss_val: 0.7283 acc_val: 0.7920 loss_test: 0.7005 acc_test: 0.8110 time: 0.0170s\n",
      "-------------------------------------------------\n",
      "Epoch: 0110 loss_train: 0.1454 acc_train: 1.0000 loss_val: 0.7267 acc_val: 0.7920 loss_test: 0.6994 acc_test: 0.8130 time: 0.0182s\n",
      "-------------------------------------------------\n",
      "Epoch: 0111 loss_train: 0.1451 acc_train: 1.0000 loss_val: 0.7283 acc_val: 0.7920 loss_test: 0.7005 acc_test: 0.8110 time: 0.0267s\n",
      "-------------------------------------------------\n",
      "Epoch: 0112 loss_train: 0.1449 acc_train: 1.0000 loss_val: 0.7271 acc_val: 0.7920 loss_test: 0.6997 acc_test: 0.8110 time: 0.0249s\n",
      "-------------------------------------------------\n",
      "Epoch: 0113 loss_train: 0.1447 acc_train: 1.0000 loss_val: 0.7271 acc_val: 0.7900 loss_test: 0.6996 acc_test: 0.8120 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0114 loss_train: 0.1445 acc_train: 1.0000 loss_val: 0.7266 acc_val: 0.7920 loss_test: 0.6992 acc_test: 0.8120 time: 0.0166s\n",
      "-------------------------------------------------\n",
      "Epoch: 0115 loss_train: 0.1442 acc_train: 1.0000 loss_val: 0.7266 acc_val: 0.7920 loss_test: 0.6992 acc_test: 0.8100 time: 0.0210s\n",
      "-------------------------------------------------\n",
      "Epoch: 0116 loss_train: 0.1440 acc_train: 1.0000 loss_val: 0.7270 acc_val: 0.7920 loss_test: 0.6994 acc_test: 0.8110 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0117 loss_train: 0.1438 acc_train: 1.0000 loss_val: 0.7261 acc_val: 0.7900 loss_test: 0.6988 acc_test: 0.8140 time: 0.0195s\n",
      "-------------------------------------------------\n",
      "Epoch: 0118 loss_train: 0.1436 acc_train: 1.0000 loss_val: 0.7265 acc_val: 0.7920 loss_test: 0.6990 acc_test: 0.8110 time: 0.0168s\n",
      "-------------------------------------------------\n",
      "Epoch: 0119 loss_train: 0.1434 acc_train: 1.0000 loss_val: 0.7259 acc_val: 0.7920 loss_test: 0.6986 acc_test: 0.8110 time: 0.0204s\n",
      "-------------------------------------------------\n",
      "Epoch: 0120 loss_train: 0.1432 acc_train: 1.0000 loss_val: 0.7263 acc_val: 0.7920 loss_test: 0.6989 acc_test: 0.8120 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0121 loss_train: 0.1430 acc_train: 1.0000 loss_val: 0.7263 acc_val: 0.7920 loss_test: 0.6988 acc_test: 0.8120 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0122 loss_train: 0.1427 acc_train: 1.0000 loss_val: 0.7251 acc_val: 0.7920 loss_test: 0.6978 acc_test: 0.8120 time: 0.0170s\n",
      "-------------------------------------------------\n",
      "Epoch: 0123 loss_train: 0.1425 acc_train: 1.0000 loss_val: 0.7252 acc_val: 0.7920 loss_test: 0.6978 acc_test: 0.8120 time: 0.0175s\n",
      "-------------------------------------------------\n",
      "Epoch: 0124 loss_train: 0.1423 acc_train: 1.0000 loss_val: 0.7250 acc_val: 0.7920 loss_test: 0.6979 acc_test: 0.8110 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0125 loss_train: 0.1421 acc_train: 1.0000 loss_val: 0.7265 acc_val: 0.7900 loss_test: 0.6989 acc_test: 0.8110 time: 0.0166s\n",
      "-------------------------------------------------\n",
      "Epoch: 0126 loss_train: 0.1419 acc_train: 1.0000 loss_val: 0.7245 acc_val: 0.7920 loss_test: 0.6973 acc_test: 0.8130 time: 0.0166s\n",
      "-------------------------------------------------\n",
      "Epoch: 0127 loss_train: 0.1417 acc_train: 1.0000 loss_val: 0.7253 acc_val: 0.7920 loss_test: 0.6978 acc_test: 0.8110 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0128 loss_train: 0.1415 acc_train: 1.0000 loss_val: 0.7240 acc_val: 0.7920 loss_test: 0.6969 acc_test: 0.8120 time: 0.0181s\n",
      "-------------------------------------------------\n",
      "Epoch: 0129 loss_train: 0.1413 acc_train: 1.0000 loss_val: 0.7254 acc_val: 0.7920 loss_test: 0.6980 acc_test: 0.8110 time: 0.0198s\n",
      "-------------------------------------------------\n",
      "Epoch: 0130 loss_train: 0.1411 acc_train: 1.0000 loss_val: 0.7240 acc_val: 0.7920 loss_test: 0.6969 acc_test: 0.8120 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0131 loss_train: 0.1410 acc_train: 1.0000 loss_val: 0.7241 acc_val: 0.7920 loss_test: 0.6969 acc_test: 0.8120 time: 0.0180s\n",
      "-------------------------------------------------\n",
      "Epoch: 0132 loss_train: 0.1408 acc_train: 1.0000 loss_val: 0.7241 acc_val: 0.7920 loss_test: 0.6969 acc_test: 0.8110 time: 0.0186s\n",
      "-------------------------------------------------\n",
      "Epoch: 0133 loss_train: 0.1406 acc_train: 1.0000 loss_val: 0.7250 acc_val: 0.7920 loss_test: 0.6976 acc_test: 0.8100 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0134 loss_train: 0.1404 acc_train: 1.0000 loss_val: 0.7237 acc_val: 0.7920 loss_test: 0.6966 acc_test: 0.8130 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0135 loss_train: 0.1402 acc_train: 1.0000 loss_val: 0.7238 acc_val: 0.7920 loss_test: 0.6966 acc_test: 0.8110 time: 0.0163s\n",
      "-------------------------------------------------\n",
      "Epoch: 0136 loss_train: 0.1400 acc_train: 1.0000 loss_val: 0.7238 acc_val: 0.7920 loss_test: 0.6966 acc_test: 0.8100 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0137 loss_train: 0.1398 acc_train: 1.0000 loss_val: 0.7235 acc_val: 0.7920 loss_test: 0.6964 acc_test: 0.8130 time: 0.0164s\n",
      "-------------------------------------------------\n",
      "Epoch: 0138 loss_train: 0.1396 acc_train: 1.0000 loss_val: 0.7236 acc_val: 0.7920 loss_test: 0.6964 acc_test: 0.8110 time: 0.0194s\n",
      "-------------------------------------------------\n",
      "Epoch: 0139 loss_train: 0.1395 acc_train: 1.0000 loss_val: 0.7229 acc_val: 0.7920 loss_test: 0.6959 acc_test: 0.8140 time: 0.0172s\n",
      "-------------------------------------------------\n",
      "Epoch: 0140 loss_train: 0.1393 acc_train: 1.0000 loss_val: 0.7230 acc_val: 0.7920 loss_test: 0.6960 acc_test: 0.8130 time: 0.0174s\n",
      "-------------------------------------------------\n",
      "Epoch: 0141 loss_train: 0.1391 acc_train: 1.0000 loss_val: 0.7236 acc_val: 0.7920 loss_test: 0.6964 acc_test: 0.8100 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0142 loss_train: 0.1389 acc_train: 1.0000 loss_val: 0.7226 acc_val: 0.7920 loss_test: 0.6956 acc_test: 0.8130 time: 0.0180s\n",
      "-------------------------------------------------\n",
      "Epoch: 0143 loss_train: 0.1388 acc_train: 1.0000 loss_val: 0.7229 acc_val: 0.7920 loss_test: 0.6958 acc_test: 0.8120 time: 0.0162s\n",
      "-------------------------------------------------\n",
      "Epoch: 0144 loss_train: 0.1386 acc_train: 1.0000 loss_val: 0.7220 acc_val: 0.7920 loss_test: 0.6952 acc_test: 0.8140 time: 0.0168s\n",
      "-------------------------------------------------\n",
      "Epoch: 0145 loss_train: 0.1384 acc_train: 1.0000 loss_val: 0.7239 acc_val: 0.7920 loss_test: 0.6964 acc_test: 0.8110 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0146 loss_train: 0.1383 acc_train: 1.0000 loss_val: 0.7215 acc_val: 0.7920 loss_test: 0.6947 acc_test: 0.8150 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0147 loss_train: 0.1381 acc_train: 1.0000 loss_val: 0.7242 acc_val: 0.7920 loss_test: 0.6965 acc_test: 0.8110 time: 0.0162s\n",
      "-------------------------------------------------\n",
      "Epoch: 0148 loss_train: 0.1380 acc_train: 1.0000 loss_val: 0.7202 acc_val: 0.7920 loss_test: 0.6938 acc_test: 0.8150 time: 0.0159s\n",
      "-------------------------------------------------\n",
      "Epoch: 0149 loss_train: 0.1378 acc_train: 1.0000 loss_val: 0.7241 acc_val: 0.7920 loss_test: 0.6965 acc_test: 0.8110 time: 0.0173s\n",
      "-------------------------------------------------\n",
      "Epoch: 0150 loss_train: 0.1376 acc_train: 1.0000 loss_val: 0.7201 acc_val: 0.7920 loss_test: 0.6937 acc_test: 0.8150 time: 0.0167s\n",
      "-------------------------------------------------\n",
      "Epoch: 0151 loss_train: 0.1374 acc_train: 1.0000 loss_val: 0.7229 acc_val: 0.7920 loss_test: 0.6955 acc_test: 0.8100 time: 0.0160s\n",
      "-------------------------------------------------\n",
      "Epoch: 0152 loss_train: 0.1373 acc_train: 1.0000 loss_val: 0.7213 acc_val: 0.7920 loss_test: 0.6945 acc_test: 0.8150 time: 0.0185s\n",
      "-------------------------------------------------\n",
      "Epoch: 0153 loss_train: 0.1371 acc_train: 1.0000 loss_val: 0.7214 acc_val: 0.7920 loss_test: 0.6945 acc_test: 0.8150 time: 0.0155s\n",
      "-------------------------------------------------\n",
      "Epoch: 0154 loss_train: 0.1370 acc_train: 1.0000 loss_val: 0.7228 acc_val: 0.7920 loss_test: 0.6955 acc_test: 0.8100 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0155 loss_train: 0.1368 acc_train: 1.0000 loss_val: 0.7193 acc_val: 0.7920 loss_test: 0.6930 acc_test: 0.8150 time: 0.0160s\n",
      "-------------------------------------------------\n",
      "Epoch: 0156 loss_train: 0.1367 acc_train: 1.0000 loss_val: 0.7233 acc_val: 0.7920 loss_test: 0.6958 acc_test: 0.8110 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0157 loss_train: 0.1365 acc_train: 1.0000 loss_val: 0.7206 acc_val: 0.7920 loss_test: 0.6939 acc_test: 0.8150 time: 0.0164s\n",
      "-------------------------------------------------\n",
      "Epoch: 0158 loss_train: 0.1363 acc_train: 1.0000 loss_val: 0.7215 acc_val: 0.7920 loss_test: 0.6944 acc_test: 0.8130 time: 0.0155s\n",
      "-------------------------------------------------\n",
      "Epoch: 0159 loss_train: 0.1362 acc_train: 1.0000 loss_val: 0.7202 acc_val: 0.7920 loss_test: 0.6935 acc_test: 0.8150 time: 0.0175s\n",
      "-------------------------------------------------\n",
      "Epoch: 0160 loss_train: 0.1360 acc_train: 1.0000 loss_val: 0.7202 acc_val: 0.7920 loss_test: 0.6935 acc_test: 0.8140 time: 0.0177s\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0161 loss_train: 0.1359 acc_train: 1.0000 loss_val: 0.7219 acc_val: 0.7920 loss_test: 0.6947 acc_test: 0.8120 time: 0.0186s\n",
      "-------------------------------------------------\n",
      "Epoch: 0162 loss_train: 0.1358 acc_train: 1.0000 loss_val: 0.7201 acc_val: 0.7920 loss_test: 0.6934 acc_test: 0.8150 time: 0.0182s\n",
      "-------------------------------------------------\n",
      "Epoch: 0163 loss_train: 0.1356 acc_train: 1.0000 loss_val: 0.7213 acc_val: 0.7920 loss_test: 0.6942 acc_test: 0.8120 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0164 loss_train: 0.1355 acc_train: 1.0000 loss_val: 0.7197 acc_val: 0.7920 loss_test: 0.6930 acc_test: 0.8150 time: 0.0171s\n",
      "-------------------------------------------------\n",
      "Epoch: 0165 loss_train: 0.1353 acc_train: 1.0000 loss_val: 0.7204 acc_val: 0.7920 loss_test: 0.6936 acc_test: 0.8150 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0166 loss_train: 0.1352 acc_train: 1.0000 loss_val: 0.7209 acc_val: 0.7920 loss_test: 0.6939 acc_test: 0.8130 time: 0.0161s\n",
      "-------------------------------------------------\n",
      "Epoch: 0167 loss_train: 0.1350 acc_train: 1.0000 loss_val: 0.7191 acc_val: 0.7920 loss_test: 0.6926 acc_test: 0.8150 time: 0.0173s\n",
      "-------------------------------------------------\n",
      "Epoch: 0168 loss_train: 0.1349 acc_train: 1.0000 loss_val: 0.7210 acc_val: 0.7920 loss_test: 0.6939 acc_test: 0.8120 time: 0.0182s\n",
      "-------------------------------------------------\n",
      "Epoch: 0169 loss_train: 0.1348 acc_train: 1.0000 loss_val: 0.7196 acc_val: 0.7920 loss_test: 0.6929 acc_test: 0.8150 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0170 loss_train: 0.1346 acc_train: 1.0000 loss_val: 0.7198 acc_val: 0.7920 loss_test: 0.6930 acc_test: 0.8150 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0171 loss_train: 0.1345 acc_train: 1.0000 loss_val: 0.7199 acc_val: 0.7920 loss_test: 0.6930 acc_test: 0.8150 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0172 loss_train: 0.1343 acc_train: 1.0000 loss_val: 0.7195 acc_val: 0.7920 loss_test: 0.6928 acc_test: 0.8150 time: 0.0218s\n",
      "-------------------------------------------------\n",
      "Epoch: 0173 loss_train: 0.1342 acc_train: 1.0000 loss_val: 0.7203 acc_val: 0.7920 loss_test: 0.6933 acc_test: 0.8140 time: 0.0198s\n",
      "-------------------------------------------------\n",
      "Epoch: 0174 loss_train: 0.1341 acc_train: 1.0000 loss_val: 0.7186 acc_val: 0.7920 loss_test: 0.6921 acc_test: 0.8150 time: 0.0174s\n",
      "-------------------------------------------------\n",
      "Epoch: 0175 loss_train: 0.1339 acc_train: 1.0000 loss_val: 0.7200 acc_val: 0.7920 loss_test: 0.6930 acc_test: 0.8140 time: 0.0169s\n",
      "-------------------------------------------------\n",
      "Epoch: 0176 loss_train: 0.1338 acc_train: 1.0000 loss_val: 0.7186 acc_val: 0.7920 loss_test: 0.6921 acc_test: 0.8140 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0177 loss_train: 0.1337 acc_train: 1.0000 loss_val: 0.7201 acc_val: 0.7920 loss_test: 0.6931 acc_test: 0.8130 time: 0.0172s\n",
      "-------------------------------------------------\n",
      "Epoch: 0178 loss_train: 0.1336 acc_train: 1.0000 loss_val: 0.7192 acc_val: 0.7920 loss_test: 0.6924 acc_test: 0.8140 time: 0.0158s\n",
      "-------------------------------------------------\n",
      "Epoch: 0179 loss_train: 0.1334 acc_train: 1.0000 loss_val: 0.7189 acc_val: 0.7920 loss_test: 0.6922 acc_test: 0.8140 time: 0.0162s\n",
      "-------------------------------------------------\n",
      "Epoch: 0180 loss_train: 0.1333 acc_train: 1.0000 loss_val: 0.7189 acc_val: 0.7920 loss_test: 0.6922 acc_test: 0.8140 time: 0.0167s\n",
      "-------------------------------------------------\n",
      "Epoch: 0181 loss_train: 0.1332 acc_train: 1.0000 loss_val: 0.7186 acc_val: 0.7920 loss_test: 0.6920 acc_test: 0.8140 time: 0.0166s\n",
      "-------------------------------------------------\n",
      "Epoch: 0182 loss_train: 0.1330 acc_train: 1.0000 loss_val: 0.7193 acc_val: 0.7900 loss_test: 0.6924 acc_test: 0.8140 time: 0.0173s\n",
      "-------------------------------------------------\n",
      "Epoch: 0183 loss_train: 0.1329 acc_train: 1.0000 loss_val: 0.7180 acc_val: 0.7920 loss_test: 0.6915 acc_test: 0.8140 time: 0.0171s\n",
      "-------------------------------------------------\n",
      "Epoch: 0184 loss_train: 0.1328 acc_train: 1.0000 loss_val: 0.7185 acc_val: 0.7920 loss_test: 0.6919 acc_test: 0.8150 time: 0.0183s\n",
      "-------------------------------------------------\n",
      "Epoch: 0185 loss_train: 0.1327 acc_train: 1.0000 loss_val: 0.7189 acc_val: 0.7920 loss_test: 0.6922 acc_test: 0.8140 time: 0.0189s\n",
      "-------------------------------------------------\n",
      "Epoch: 0186 loss_train: 0.1326 acc_train: 1.0000 loss_val: 0.7176 acc_val: 0.7920 loss_test: 0.6913 acc_test: 0.8140 time: 0.0212s\n",
      "-------------------------------------------------\n",
      "Epoch: 0187 loss_train: 0.1324 acc_train: 1.0000 loss_val: 0.7195 acc_val: 0.7920 loss_test: 0.6925 acc_test: 0.8130 time: 0.0192s\n",
      "-------------------------------------------------\n",
      "Epoch: 0188 loss_train: 0.1323 acc_train: 1.0000 loss_val: 0.7170 acc_val: 0.7920 loss_test: 0.6908 acc_test: 0.8140 time: 0.0192s\n",
      "-------------------------------------------------\n",
      "Epoch: 0189 loss_train: 0.1322 acc_train: 1.0000 loss_val: 0.7185 acc_val: 0.7920 loss_test: 0.6917 acc_test: 0.8130 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0190 loss_train: 0.1321 acc_train: 1.0000 loss_val: 0.7178 acc_val: 0.7920 loss_test: 0.6913 acc_test: 0.8140 time: 0.0210s\n",
      "-------------------------------------------------\n",
      "Epoch: 0191 loss_train: 0.1320 acc_train: 1.0000 loss_val: 0.7183 acc_val: 0.7920 loss_test: 0.6916 acc_test: 0.8140 time: 0.0179s\n",
      "-------------------------------------------------\n",
      "Epoch: 0192 loss_train: 0.1319 acc_train: 1.0000 loss_val: 0.7173 acc_val: 0.7920 loss_test: 0.6908 acc_test: 0.8140 time: 0.0187s\n",
      "-------------------------------------------------\n",
      "Epoch: 0193 loss_train: 0.1317 acc_train: 1.0000 loss_val: 0.7177 acc_val: 0.7920 loss_test: 0.6912 acc_test: 0.8140 time: 0.0210s\n",
      "-------------------------------------------------\n",
      "Epoch: 0194 loss_train: 0.1316 acc_train: 1.0000 loss_val: 0.7187 acc_val: 0.7900 loss_test: 0.6919 acc_test: 0.8130 time: 0.0234s\n",
      "-------------------------------------------------\n",
      "Epoch: 0195 loss_train: 0.1315 acc_train: 1.0000 loss_val: 0.7161 acc_val: 0.7920 loss_test: 0.6900 acc_test: 0.8140 time: 0.0178s\n",
      "-------------------------------------------------\n",
      "Epoch: 0196 loss_train: 0.1314 acc_train: 1.0000 loss_val: 0.7189 acc_val: 0.7920 loss_test: 0.6919 acc_test: 0.8120 time: 0.0194s\n",
      "-------------------------------------------------\n",
      "Epoch: 0197 loss_train: 0.1313 acc_train: 1.0000 loss_val: 0.7170 acc_val: 0.7920 loss_test: 0.6906 acc_test: 0.8140 time: 0.0172s\n",
      "-------------------------------------------------\n",
      "Epoch: 0198 loss_train: 0.1312 acc_train: 1.0000 loss_val: 0.7171 acc_val: 0.7920 loss_test: 0.6906 acc_test: 0.8140 time: 0.0190s\n",
      "-------------------------------------------------\n",
      "Epoch: 0199 loss_train: 0.1311 acc_train: 1.0000 loss_val: 0.7179 acc_val: 0.7900 loss_test: 0.6912 acc_test: 0.8140 time: 0.0182s\n",
      "-------------------------------------------------\n",
      "Epoch: 0200 loss_train: 0.1309 acc_train: 1.0000 loss_val: 0.7172 acc_val: 0.7920 loss_test: 0.6907 acc_test: 0.8140 time: 0.0164s\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "training_log = []\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "temp_val_loss = 999999\n",
    "temp_val_acc = 0\n",
    "temp_test_loss = 0\n",
    "temp_test_acc = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "\n",
    "    epo, trainloss, trainacc, valloss, valacc, testloss, testacc, epotime, output, fp1, fp2 = train(epoch)\n",
    "    training_log.append([epo, trainloss, trainacc, valloss, valacc, testloss, testacc, epotime])\n",
    "\n",
    "    if valacc >= temp_val_acc:\n",
    "        temp_val_loss = valloss\n",
    "        temp_val_acc = valacc\n",
    "        temp_test_loss = testloss\n",
    "        temp_test_acc = testacc\n",
    "        propagation_feats = fp1\n",
    "        before_softmax = fp2\n",
    "        after_softmax = output\n",
    "        \n",
    "        if model_name == 'AFGNN':\n",
    "            temp_weight = torch.softmax(model.state_dict()['gc1.linear_weight'].data,dim=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Total time elapsed: 4.1452s\n",
      "Best result: val_loss= 0.8655314445495605 test_loss= 0.8287785053253174 test_acc= 0.816\n"
     ]
    }
   ],
   "source": [
    "# print result\n",
    "\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "print(\"Best result:\",\n",
    "          \"val_loss=\",temp_val_loss,\n",
    "            \"test_loss=\",temp_test_loss,\n",
    "             \"test_acc=\",temp_test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
